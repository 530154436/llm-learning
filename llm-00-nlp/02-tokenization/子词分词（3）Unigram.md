### 3.4 Unigram
Unigram Language Model (ULM)模型是Kudo提出的。当时主要是为了解决机器翻译中分词的问题。作者使用一种叫做marginalized likelihood的方法来建模翻译问题，考虑到不同分词结果对最终翻译结果的影响，引入了分词概率。

与WordPiece一样，Unigram Language Model(ULM)同样使用语言模型来挑选子词。不同之处在于，BPE和WordPiece算法的词表大小都是从小到大变化，属于增量法。而Unigram Language Model则是**减量法**,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。ULM算法考虑了句子的不同分词可能，因而能够输出带概率的多个子词分段。

> **参考文献：**
> - [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/pdf/1804.10959)


对于句子 $S$ , $\vec x=(x_{1},x_{2},...,x_{m})$ 为句子的一个分词结果，由 $m$ 个子词组成。并且假设每个子词都与其之前的子词独立。所以，当前分词下句子 $S$ 的似然值可以表示为：

$$
P(\vec x)=\prod_{i=1}^m{P(x_{i})}
$$

对于句子 $S$ ，挑选似然值最大的作为分词结果，则可以表示为

$$
x^{*}=arg max_{x \in U(x)} P(\vec x)
$$

其中 $U(x)$ 包含了句子的所有分词结果。在实际应用中，词表大小有上万个，直接罗列所有可能的分词组合不具有操作性。针对这个问题，可通过`维特比算法`得到 $x^*$ 来解决。

那怎么求解每个子词的概率 $P(x_{i})$ 呢？ULM通过EM算法来估计。假设当前词表 $V$ , 则 $M$ 步最大化的对象是如下似然函数：

$$
L=\sum_{s=1}^{|D|}log(P(X^{(s)}))=\sum_{s=1}^{|D|}log(\sum_{x \in U(X^{(s)})}P(x))
$$

其中， $|D|$ 是语料库中语料数量。上述公式的一个直观理解是，将语料库中所有句子的所有分词组合形成的概率相加。
但是，初始时，词表 $V$ 并不存在。因而，ULM算法采用不断迭代的方法来构造词表以及求解分词概率：

1. **初始化词汇表**<br>
   一般，可用语料中的所有字符加上常见的子字符串初始化词表，也可以通过BPE算法初始化。
2. 针对当前词表，用**EM算法**求解每个子词在语料上的概率。
3. 对于每个子词，计算当该子词被从词表中移除时，总的loss降低了多少，记为该子词的**loss**。
4. 将子词按照loss大小进行排序，**丢弃一定比例loss最小的子词**(比如20%)，保留下来的子词生成新的词表。<br>
   这里需要注意的是，单字符不能被丢弃，这是为了避免OOV情况。
5. 重复步骤2到4，直到词表大小减少到设定范围。

可以看出，ULM会保留那些以较高频率出现在很多句子的分词结果中的子词，因为这些子词如果被丢弃，其损失会很大。有以下优点：
+ 使用的训练算法可以利用所有可能的分词结果，这是通过data sampling算法实现的。
+ 提出一种基于语言模型的分词算法，这种语言模型可以给多种分词结果赋予概率，从而可以学到其中的噪声。

示例1 计算每个子词在语料上的概率、根据似然值对语料库中的每个单词进行分词：
```
假设语料库预分词后得到的单词及其频次的集合：
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)

取这个语料库中所有的子串作为初始词汇库：
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]

词汇库中所有可能出现子词的频率（所有频率之和为 210）：
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)

为了对一个给定的单词进行分词，我们会查看所有可能的分词组合，并根据 Unigram 模型计算出每种可能的概率。
由于所有的分词都被视为独立的，因此这个单词分词的概率就是每个子词概率的乘积。以 "pug" 为例，我们得到的各种可能分词方式的概率如下：
P(["p","u","g"]) = P("p") × P("u") × P("g") = (17/210) *  (36/210) * (20/210) = 0.001321
P(["pu","g"]) = P("pu") × P("g") = (17/210) * (20/210) = 0.007709
P(["p","ug"]) = P("p") × P("ug") = (17/210) * (20/210) = 0.007709
因此， "pug" 将被分词为 ["p", "ug"] 或 ["pu", "g"]。

在这个例子中，找出所有可能的分词方式并计算其概率是容易的，但在语料库比较大的情况下有些困难。有一个经典的算法可以用来计算这个概率，叫做 `Viterbi 算法` 。
通过创建一个图来表示一个给定单词的所有可能分词情况，其中：如果从字符 `a` 到字符 `b` 的子词存在于词汇表中，则在图中存在一条从 `a` 到 `b` 的边。每条边代表从 `a` 到 `b` 进行切分的概率。

为了在该图中找到得分最高的路径（即最优分词），`Viterbi算法`会执行以下步骤：
1. **初始化**：为句子中的每个位置确定最佳得分分割点。
2. **遍历与计算**：从句子的开始到结束，对于每个位置，遍历所有以当前位置结束的子词，并结合这些子词起始位置的最佳得分来计算最高得分。
3. **回溯路径**：一旦到达句子的末尾，只需回溯之前记录的路径，即可得到最终的最优路径，即得分最高的分词组合。
以 "unhug" 为例，对于每个位置，最佳切分子词的分数如下：
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

示例2 确定分词结果后，计算整个语料库的loss，以及计算移除每个子词(token)对损失值的影响：
```
语料库中的每个词都有一个分数，损失（loss）值是这些分数的负对数似然——即所有词的语料库中所有词的 -log(P(word)) 总和
每个单词的分词及其相应的得分如下：
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)

因此，损失值（loss）是：
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8

需要计算移除每个 token 对损失值的影响。例如，“pug”可以被分词为 ["pu", "g"] ，也可以被分词为 ["p", "ug"] ，获得的分数是相同的。
因此，去除词汇表中的 "pu" 损失值还会是一样的。但是，去除 "hug" 之后，损失会变得更糟，因为 "hug" 和 "hugs" 的 tokenization 会变成：
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
这些变化将导致损失增加：
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
因此， "pu" tokens 可能会从词汇表中移除，但 "hug" 则不会。
```

## 参考引用
[1] [预训练分词Subword](https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/subword.html#unigram-language-model-ulm)<br>
[2] [transformers-Unigram tokenization 算法](https://huggingface.co/learn/llm-course/zh-CN/chapter6/7?fw=pt)<br>
