## 一、维特比算法
维特比算法（Viterbi algorithm）是一种动态规划算法。用于寻找最有可能产生观测事件序列的维特比路径——隐含状态序列，特别是在马尔可夫信息源上下文和隐马尔可夫模型中。
核心思想是：
+ **动态规划递推**：通过逐步计算每个位置的最优子路径，避免重复计算。
+ **路径回溯**：通过记录每个节点的最优前驱路径，最终回溯得到全局最优解。

### 1.1 分词
#### 1.1.1 N-元语言模型
 **n元语言模型**是一种概率模型，用于预测一个句子的概率。它基于前 $n-1$ 个词来预测第 $n$ 个词的概率。该模型的基本假设是当前词的出现仅依赖于前面的 $n-1$ 个词。


- **Unigram 模型**：考虑单个词的概率，忽略上下文信息。<br>
  概率公式： $P(w_i)$ <br>
  句子的概率为每个单词概率的乘积： $P(S)=\prod_{i=1}^{n} P(w_i)$

- **Bigram 模型**：考虑当前词与其直接前一个词共同出现的概率。<br>
  条件概率公式： $P(w_i|w_{i-1})$ <br>
  句子的概率为相邻词对条件概率的乘积： $P(S)=\prod_{i=2}^{n}P(w_i|w_{i-1})$

似然函数衡量给定数据集在特定参数下的可能性。对于语言模型，这通常涉及最大化训练语料库上的似然。
- Unigram： $\mathcal{L} = \prod_{s \in D} \prod_{w \in s} P(w)$
- Bigram： $\mathcal{L} = \prod_{s \in D} \prod_{i=2}^{|s|} P(w_i|w_{i-1})$

为了便于优化，我们通常使用负对数似然作为损失函数：
- Unigram： $\text{Loss} = -\sum_{s \in D} \sum_{w \in s} \log(P(w))$
- Bigram：  $\text{Loss} = -\sum_{s \in D} \sum_{i=2}^{|s|} \log(P(w_i|w_{i-1}))$

#### 1.1.2 维特比算法求解
核心思路：通过动态规划（DP）维护每个位置的最优分词路径及累积概率，结合子词概率进行计算。
+ 数组dp[i]: 表示到达位置 i 时的最大概率。 
+ 数组path[i]: 表示到位置 i 时的分词路径。例如，path[i+1]=j表示以j为开始下标、i为结束下标的词元。

具体步骤如下：
1. 初始化：dp[0] 初始概率为0，路径为空。
2. 递推计算：遍历每个位置 i，尝试所有可能的前驱位置 j，计算候选词的条件概率。
3. 路径记录：保存最大概率对应的分词路径。
4. 回溯最优解：最终 dp[n] 中的路径即为最优分词结果。

Python实现示例：
```python
def viterbi(word: str, model: Dict[str, float]) -> Tuple[List[str], int]:
    """
    Viterbi算法（动态规划）
    :param word: 字符串
    :param model:  子词对应的概率
    :return:
    """
    n = len(word)
    dp = [float("inf")] * (n + 1)
    path = [-1] * (n + 1)
    dp[0] = 0

    # 动态规划求解
    for i in range(1, n + 1):
        for j in range(i):
            token = word[j: i]
            if token in model:
                score = dp[j] + model[token]
                if score < dp[i]:
                    dp[i] = score
                    path[i] = j
    if path[-1] == -1:
        return ["<unk>"], None

    # 回溯得到最优切分方式
    optimal_split = []
    i = n
    while i > 0:
        prev = path[i]
        optimal_split.append(word[prev:i])
        i = prev
    optimal_split.reverse()
    return optimal_split, dp[n]


_word = "thought"
_model = {'h': 3.45, 't': 3.01, 'u': 3.85, 'g': 4.04, 'o': 3.08, 'th': 4.55, 'ou': 4.55}
_optimal_split, _score = viterbi(_word, _model)
# 最优切分方式: ['th', 'ou', 'g', 'h', 't']
# 最小得分之和: 19.6
print("最优切分方式:", _optimal_split)
print("最小得分之和:", _score)
```
图示：在效率方面相对于粗暴地遍历所有路径，viterbi 维特比算法到达每一列的时候都会删除不符合最短路径要求的路径，大大降低时间复杂度。（红色线保留、黑色线删除）

<img src="../images/01-tokenization/维特比分词.png" width="60%" height="60%" alt="">

## 二、EM算法


## 参考
[1] [文本挖掘的分词原理](https://www.cnblogs.com/pinard/p/6677078.html)<br>
[2] [如何通俗地讲解 viterbi 算法？](https://www.zhihu.com/question/20136144/answer/763021768?utm_psn=1895852064111829755)<br>
[3] []()<br>

