### 3.4 Unigram
与 BPE 分词和 WordPiece 分词不同，Unigram 分词方法从语料库的一组足够大的字符串或词元初始集合开始，迭代地删除其中的词元，直到达到预期的词表大小。
它假设从当前词表中删除某个词元，并计算训练语料的似然增加情况，以此来作为选择标准。
这个步骤是基于一个训练好的一元语言模型来进行的。
为估计一元语言模型，它采用期望最大化（Expectation–Maximization, EM）算法：
在每次迭代中，首先基于旧的语言模型找到当前最优的分词方式，然后重新估计一元概率从而更新语言模型。
这个过程中一般使用动态规划算法（即`维特比算法`，Viterbi Algorithm）来高效地找到语言模型对词汇的最优分词方式。采用这种分词方法的代表性模型包括 T5 和 mBART。
