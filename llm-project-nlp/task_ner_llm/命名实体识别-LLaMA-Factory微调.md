## ä¸€ã€ç¯å¢ƒé…ç½®
dockeré•œåƒ
```
FROM nvidia/cuda:12.2.2-cudnn8-runtime-ubuntu22.04
```
CUDAç‰ˆæœ¬
```shell
nvcc -V
# Cuda compilation tools, release 12.2, V12.2.140
# Build cuda_12.2.r12.2/compiler.33191640_0
```
LLaMa-Factoryç‰ˆæœ¬
```shell
llamafactory-cli version
#----------------------------------------------------------
#| Welcome to LLaMA Factory, version 0.9.2                |
#| Project page: https://github.com/hiyouga/LLaMA-Factory |
#----------------------------------------------------------
```
> å®˜æ–¹æ–‡æ¡£æ˜¯ git clone æ•´ä¸ªé¡¹ç›®ä»¥ä¾¿æ”¯æŒæœ€æ–°çš„æ¨¡å‹ã€‚

## äºŒã€å‡†å¤‡å·¥ä½œ
### 2.1 åŸºåº§æ¨¡å‹ä¸‹è½½
```python
from modelscope import snapshot_download
snapshot_download('Qwen/Qwen2.5-7B-Instruct', cache_dir='models', revision='master')
```
### 2.2 æ•°æ®é›†æ„å»ºï¼ˆalpacaæ ¼å¼ï¼‰
åŸå§‹æ•°æ®
```json lines
{
	"text": "åŒ—äº¬åŸ.",  // è¡¨ç¤ºåŸå§‹æ–‡æœ¬
	"label": {
		"NT": {"åŒ—äº¬åŸ": [[0, 2]]}  // "label" è¡¨ç¤ºå®ä½“ç±»å‹ï¼ˆå¦‚ NTï¼‰åŠå…¶å¯¹åº”çš„å®ä½“åç§°å’Œå‡ºç°çš„ä½ç½®ç´¢å¼•ã€‚
	}
}
```
è½¬æ¢ä¸ºalpacaæ ¼å¼ï¼šdata/dataset/alpaca/alpaca_clue_train.json
```
[
  {
    "instruction": "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬å®ä½“è¯†åˆ«é¢†åŸŸçš„ä¸“å®¶ï¼Œè¯·ä»ç»™å®šçš„å¥å­ä¸­è¯†åˆ«å¹¶æå–å‡ºä»¥ä¸‹æŒ‡å®šç±»åˆ«çš„å®ä½“ã€‚\n\n<å®ä½“ç±»åˆ«é›†åˆ>\nname, organization, scene, company, movie, book, government, position, address, game\n\n<ä»»åŠ¡è¯´æ˜>\n1. ä»…æå–å±äºä¸Šè¿°ç±»åˆ«çš„å®ä½“ï¼Œå¿½ç•¥å…¶ä»–ç±»å‹çš„å®ä½“ã€‚\n2. ä»¥jsonæ ¼å¼è¾“å‡ºï¼Œå¯¹äºæ¯ä¸ªè¯†åˆ«å‡ºçš„å®ä½“ï¼Œè¯·æä¾›ï¼š\n   - label: å®ä½“ç±»å‹ï¼Œå¿…é¡»ä¸¥æ ¼ä½¿ç”¨åŸå§‹ç±»å‹æ ‡è¯†ï¼ˆä¸å¯æ›´æ”¹ï¼‰\n   - text: å®ä½“åœ¨åŸæ–‡ä¸­çš„ä¸­æ–‡å†…å®¹\n\n<è¾“å‡ºæ ¼å¼è¦æ±‚>\n```json\n[{{\"label\": \"å®ä½“ç±»åˆ«\", \"text\": \"å®ä½“åç§°\"}}]\n```",
    "input": "æµ™å•†é“¶è¡Œä¼ä¸šä¿¡è´·éƒ¨å¶è€æ¡‚åšå£«åˆ™ä»å¦ä¸€ä¸ªè§’åº¦å¯¹äº”é“é—¨æ§›è¿›è¡Œäº†è§£è¯»ã€‚å¶è€æ¡‚è®¤ä¸ºï¼Œå¯¹ç›®å‰å›½å†…å•†ä¸šé“¶è¡Œè€Œè¨€ï¼Œ",
    "output": "[{\"label\": \"name\", \"text\": \"å¶è€æ¡‚\"}, {\"label\": \"company\", \"text\": \"æµ™å•†é“¶è¡Œ\"}]"
  }
]
```
æ³¨å†Œè‡ªå®šä¹‰æ•°æ®é›†ï¼Œå°†æ•°æ®é›†æ·»åŠ åˆ°å…¨å±€é…ç½®ï¼šdata/dataset/alpaca/dataset_info.json
```
{
  "alpaca_clue_train": {
    "file_name": "alpaca_clue_train.json"
  }
}
```

## ä¸‰ã€è®­ç»ƒæµç¨‹
### 3.1 é…ç½®æ–‡ä»¶
conf/Qwen2.5-7B-Instruct-clue-ner-lora-sft.yaml
```
### model
model_name_or_path: ../model_hub/Qwen2.5-7B-Instruct
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
flash_attn: auto
lora_rank: 8
lora_target: all

### dataset
dataset_dir: data/dataset/alpaca  # å­˜å‚¨æ•°æ®é›†çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
dataset: alpaca_clue_train
template: qwen  # Qwen (1-2.5)
cutoff_len: 1024  # è¾“å…¥çš„æœ€å¤§ token æ•°ï¼Œè¶…è¿‡è¯¥é•¿åº¦ä¼šè¢«æˆªæ–­ã€‚
max_samples: 15000  # æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°ï¼šè®¾ç½®åï¼Œæ¯ä¸ªæ•°æ®é›†çš„æ ·æœ¬æ•°å°†è¢«æˆªæ–­è‡³æŒ‡å®šçš„ max_samplesã€‚
overwrite_cache: true  # æ˜¯å¦è¦†ç›–ç¼“å­˜çš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ã€‚
preprocessing_num_workers: 16
dataloader_num_workers: 8

### output
output_dir: data/outputs/Qwen2.5-7B-Instruct-clue-ner-lora-sft
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true
save_only_model: false

### train
per_device_train_batch_size: 4  # æ¯è®¾å¤‡è®­ç»ƒæ‰¹æ¬¡å¤§å°, é»˜è®¤ 8
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

### eval
#eval_dataset:
val_size: 0.1
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 500
```
### 3.2 è®­ç»ƒè¿‡ç¨‹
å¯åŠ¨è®­ç»ƒï¼š
```shell
llamafactory-cli train conf/Qwen2.5-7B-Instruct-lora-sft.yaml
```
è®­ç»ƒæ—¶æ¡†æ¶ä¼šå°†alpacaæ ¼å¼æ•°æ®é›†è½¬æ¢ä¸ºä¸åŒå¤§æ¨¡å‹çš„å¯¹è¯æ¨¡æ¿ï¼Œæ•°æ®ç¤ºä¾‹å¦‚ä¸‹ï¼š
+ `inputs`
```
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬å®ä½“è¯†åˆ«é¢†åŸŸçš„ä¸“å®¶ï¼Œè¯·ä»ç»™å®šçš„å¥å­ä¸­è¯†åˆ«å¹¶æå–å‡ºä»¥ä¸‹æŒ‡å®šç±»åˆ«çš„å®ä½“ã€‚

<å®ä½“ç±»åˆ«é›†åˆ>
name, organization, scene, company, movie, book, government, position, address, game

<ä»»åŠ¡è¯´æ˜>
1. ä»…æå–å±äºä¸Šè¿°ç±»åˆ«çš„å®ä½“ï¼Œå¿½ç•¥å…¶ä»–ç±»å‹çš„å®ä½“ã€‚
2. ä»¥jsonæ ¼å¼è¾“å‡ºï¼Œå¯¹äºæ¯ä¸ªè¯†åˆ«å‡ºçš„å®ä½“ï¼Œè¯·æä¾›ï¼š
   - label: å®ä½“ç±»å‹ï¼Œå¿…é¡»ä¸¥æ ¼ä½¿ç”¨åŸå§‹ç±»å‹æ ‡è¯†ï¼ˆä¸å¯æ›´æ”¹ï¼‰
   - text: å®ä½“åœ¨åŸæ–‡ä¸­çš„ä¸­æ–‡å†…å®¹

<è¾“å‡ºæ ¼å¼è¦æ±‚>
``json
[{{"label": "å®ä½“ç±»åˆ«", "text": "å®ä½“åç§°"}}]
``

æµ™å•†é“¶è¡Œä¼ä¸šä¿¡è´·éƒ¨å¶è€æ¡‚åšå£«åˆ™ä»å¦ä¸€ä¸ªè§’åº¦å¯¹äº”é“é—¨æ§›è¿›è¡Œäº†è§£è¯»ã€‚å¶è€æ¡‚è®¤ä¸ºï¼Œå¯¹ç›®å‰å›½å†…å•†ä¸šé“¶è¡Œè€Œè¨€ï¼Œ<|im_end|>
<|im_start|>assistant
[{"label": "name", "text": "å¶è€æ¡‚"}, {"label": "company", "text": "æµ™å•†é“¶è¡Œ"}]<|im_end|>
```
+ `labels`
```
[{"label": "name", "text": "å¶è€æ¡‚"}, {"label": "company", "text": "æµ™å•†é“¶è¡Œ"}]<|im_end|>
```
GPUæ˜¾å­˜å ç”¨æƒ…å†µï¼š
```
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  Off  | 00000000:65:00.0 Off |                    0 |
| N/A   67C    P0   193W / 250W |  30756MiB / 40536MiB |     48%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
```
è®­ç»ƒæ—¥å¿—ï¼š
```
[INFO|trainer.py:2405] 2025-05-30 02:56:21,267 >> ***** Running training *****
[INFO|trainer.py:2406] 2025-05-30 02:56:21,268 >>   Num examples = 9,673
[INFO|trainer.py:2407] 2025-05-30 02:56:21,268 >>   Num Epochs = 3
[INFO|trainer.py:2408] 2025-05-30 02:56:21,268 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2411] 2025-05-30 02:56:21,268 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2412] 2025-05-30 02:56:21,268 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2413] 2025-05-30 02:56:21,268 >>   Total optimization steps = 906
[INFO|trainer.py:2414] 2025-05-30 02:56:21,274 >>   Number of trainable parameters = 20,185,088
```
losså˜åŒ–


### LLaMA-Factory
å®˜ç½‘æ•™ç¨‹ï¼šhttps://llamafactory.readthedocs.io/zh-cn/latest/getting_started/sft.html
æ•°æ®é›†å‚æ•°ï¼šhttps://github.com/hiyouga/LLaMA-Factory/blob/main/src/llamafactory/hparams/data_args.py#L38
å‚æ•°é…ç½®ï¼šhttps://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html
æŸ¥çœ‹æç¤ºè¯æ¨¡æ¿ï¼šhttps://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md

å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆä¸€ï¼‰-èƒŒæ™¯ã€å‚æ•°é«˜æ•ˆå¾®è°ƒç®€ä»‹
https://github.com/liguodongiot/llm-action?tab=readme-ov-file
https://zhuanlan.zhihu.com/p/635152813

Qwen2.5å¤§æ¨¡å‹å¾®è°ƒå®æˆ˜ï¼šåŒ»ç–—å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ï¼ˆå®Œæ•´ä»£ç ï¼‰
https://zhuanlan.zhihu.com/p/19682001982

åŸºäº Qwen2.5-0.5B å¾®è°ƒè®­ç»ƒ Ner å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡
https://blog.csdn.net/qq_43692950/article/details/142631780

qwen3 finetune
https://qwen.readthedocs.io/zh-cn/latest/training/llama_factory.html


åœ¨è¿›è¡Œæ¨¡å‹å¾®è°ƒæ—¶ï¼Œæ˜¯å¦åº”è¯¥å°† system æ¶ˆæ¯ä¹ŸåŒ…å«åœ¨è®­ç»ƒæ•°æ®ä¸­ï¼Ÿ
ğŸ¯ æ§åˆ¶è§’è‰²ä¸€è‡´æ€§	åŒ…å« system å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´ç¨³å®šåœ°è®°ä½è‡ªå·±çš„ä»»åŠ¡è§’è‰²ï¼ˆæ¯”å¦‚ï¼šå®ä½“è¯†åˆ«ä¸“å®¶ï¼‰ï¼Œé¿å…åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´æ··æ·†ã€‚
ğŸ¤– æ›´è´´è¿‘å®é™…ä½¿ç”¨åœºæ™¯	å¦‚æœä½ åœ¨éƒ¨ç½²æˆ–æ¨ç†é˜¶æ®µä½¿ç”¨äº† system æ¥è®¾å®šè§’è‰²ï¼Œé‚£ä¹ˆåœ¨è®­ç»ƒæ—¶ä¹Ÿåº”è¯¥ä¿ç•™å®ƒï¼Œè¿™æ ·è®­ç»ƒå’Œæ¨ç†çš„ä¸Šä¸‹æ–‡æ‰ä¸€è‡´ã€‚
ğŸ§© æå‡æ³›åŒ–èƒ½åŠ›	æ¨¡å‹èƒ½æ›´å¥½åœ°ç†è§£â€œæˆ‘æ˜¯ä¸€ä¸ªå®ä½“è¯†åˆ«åŠ©æ‰‹â€ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªé€šç”¨é—®ç­”æ¨¡å‹ï¼Œä»è€Œåœ¨æ–°å¥å­ä¸Šè¡¨ç°æ›´å‡†ç¡®ã€‚
ğŸ§ª å¤šä»»åŠ¡è®­ç»ƒæ”¯æŒ	å¦‚æœä½ æœªæ¥è®¡åˆ’è®­ç»ƒå¤šä¸ªä»»åŠ¡ï¼ˆå¦‚å®ä½“è¯†åˆ« + å…³ç³»æŠ½å–ï¼‰ï¼Œå¯ä»¥é€šè¿‡ä¸åŒçš„ system æ¥åŒºåˆ†ä»»åŠ¡ç±»å‹ï¼Œæå‡æ¨¡å‹å¯æ§æ€§ã€‚
transformer gpt è¾“å…¥çš„maskå’Œbertçš„maskå¥½åƒæ˜¯ç›¸åçš„ï¼Ÿ
