### 基于Transformers+peft框架
利用大模型做NER实践(总结版)
https://mp.weixin.qq.com/s/LBlzFm8wxK7Aj7YXhCoXgQ
https://github.com/cjymz886/LLM-NER

指令微调-命名实体识别
博客：https://blog.csdn.net/SoulmateY/article/details/139831606
代码：https://github.com/Zeyi-Lin/LLM-Finetune

05-Qwen3-8B-LoRA及SwanLab可视化记录.md
https://github.com/datawhalechina/self-llm/blob/master/models/Qwen3/05-Qwen3-8B-LoRA%E5%8F%8ASwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md

### LLaMA-Factory
官网教程：https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/sft.html

数据集参数：https://github.com/hiyouga/LLaMA-Factory/blob/main/src/llamafactory/hparams/data_args.py#L38
参数配置：https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html
查看提示词模板：https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md

大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介
https://github.com/liguodongiot/llm-action?tab=readme-ov-file
https://zhuanlan.zhihu.com/p/635152813

Qwen2.5大模型微调实战：医疗命名实体识别任务（完整代码）
https://zhuanlan.zhihu.com/p/19682001982

基于 Qwen2.5-0.5B 微调训练 Ner 命名实体识别任务
https://blog.csdn.net/qq_43692950/article/details/142631780

qwen3 finetune
https://qwen.readthedocs.io/zh-cn/latest/training/llama_factory.html


在进行模型微调时，是否应该将 system 消息也包含在训练数据中？
🎯 控制角色一致性	包含 system 可以帮助模型更稳定地记住自己的任务角色（比如：实体识别专家），避免在不同任务之间混淆。
🤖 更贴近实际使用场景	如果你在部署或推理阶段使用了 system 来设定角色，那么在训练时也应该保留它，这样训练和推理的上下文才一致。
🧩 提升泛化能力	模型能更好地理解“我是一个实体识别助手”，而不是一个通用问答模型，从而在新句子上表现更准确。
🧪 多任务训练支持	如果你未来计划训练多个任务（如实体识别 + 关系抽取），可以通过不同的 system 来区分任务类型，提升模型可控性。
transformer gpt 输入的mask和bert的mask好像是相反的？
