## 常用镜像构建和启动

### 1、镜像Tag标识的含义
base/cuda: 包括 CUDA 运行时<br>
runtime: 在 base 的基础上，新增了 CUDA math 库和 NCCL、cuDNN 运行时<br>
devel: 在 runtime 的基础上，新增了头文件和用于构建 CUDA 镜像的开发工具，对于多阶段构建特别有用<br>
cuddn: 在上面基础上，新增了 cuDNN 神经网络加速库<br>
py3: Python 3 环境<br>

### 2、常用镜像
环境配置：<br>
NVIDIA A100、CUDA 12.2<br>

| 镜像名称                                          | 大小     | Python版本 | 备注 |
|-----------------------------------------------|--------|----------|----|
| pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime | 6.48GB | 3.10.9   |    |
| pytorch/pytorch:2.1.2-cuda12.1-cudnn8-runtime | 7.22GB |          |    |
| pytorch/pytorch:2.4.0-cuda12.1-cudnn9-devel   |        | 3.11.9   |    |
| pytorch/pytorch:2.4.0-cuda12.1-cudnn9-runtime | 8.06GB | 3.11.9   |    |
| nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04   |        | 3.10.12  |    |
| nvidia/cuda:12.2.2-cudnn8-runtime-ubuntu22.04 |        | 3.10.12  |    |

> [PyTorch官方镜像](https://hub.docker.com/r/pytorch/pytorch/tags)<br>
> [Nvidia-Cuda官方镜像](https://hub.docker.com/r/nvidia/cuda/tags)

### 3、镜像构建与运行
+ 拉取镜像
```shell
# docker pull 一直有问题
# Pytorch镜像启动vllm==0.6.3会有些奇怪的问题：https://github.com/vllm-project/vllm/issues/8367
docker pull pytorch/pytorch:2.4.0-cuda12.1-cudnn9-runtime
docker pull pytorch/pytorch:2.4.0-cuda12.1-cudnn9-devel
docker pull nvidia/cuda:12.2.2-cudnn8-runtime-ubuntu22.04 --platform=linux/amd64
docker pull nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04 --platform=linux/amd64

# 查看镜像
# docker run -it nvidia/cuda:12.2.2-cudnn8-runtime-ubuntu22.04 /bin/bash
```

+ 导入导出镜像
```shell
docker images
# docker save -o <保存路径>_<镜像文件>.tar <镜像名称>:<标签>
docker save -o cuda_12.2.2-cudnn8-runtime-ubuntu22.04.tar nvidia/cuda:12.2.2-cudnn8-runtime-ubuntu22.04
docker load -i cuda_12.2.2-cudnn8-runtime-ubuntu22.04.tar
```

+ 构建和启动镜像
```shell

# (1) 构建镜像
# 使用的镜像：FROM pytorch/pytorch:2.4.0-cuda12.1-cudnn9-runtime
docker build . -f Dockerfile -t harbor.qdyip.vip/algorithm-model/wzalgo_publicsentiment_extract:v3

# 操作镜像
docker images | grep wzalgo_publicsentiment_extract
docker rmi 08ba43b85201

# 验证镜像的环境是否正常
docker run --gpus all -it harbor.qdyip.vip/algorithm-model/wzalgo_publicsentiment_extract:v3 /bin/bash
# root@3f7c9c8a88f1:/workspace# python
# >>> import torch
# >>> torch.__version__
# '2.4.0'

# 操作容器
docker ps -a | grep wzalgo_publicsentiment_extract
docker stop de98861baeac
docker start -p 3367:3367

docker rm de98861baeac
docker logs -f --tail=100 de98861baeac
docker exec -it 0b02b211721d /bin/bash 

# (2) 通过挂载外部数据的方式读取模型文件和代码: 相当于在容器里运行的是/data/nfs/下的代码+模型，而不是原来的代码
# -d: 后台运行
docker run --name wzalgo_publicsentiment_extract_v3 \
-p 30769:8000 -p 30019:8080 \
-v /data/devNFS:/data/devNFS \
-w /data/devNFS/zhengchubin/llm-learning/ \
--gpus all \
-it harbor.qdyip.vip/algorithm-model/wzalgo_publicsentiment_extract:v3 /bin/bash

# 启动
/bin/bash llm_app_deploy/app_vllm.sh

# (3) 推送到hub
docker push harbor.qdyip.vip/algorithm-model/wzalgo_publicsentiment_extract:v3
```

### 参考引用
[1] [常用 AI 基础镜像及启动命令](https://www.chenshaowen.com/blog/common-ai-base-images-and-run-command.html)<br>
[2] [混合云离线GPU模型服务部署使用补充说明](https://cf.qizhidao.com/pages/viewpage.action?pageId=100034817)