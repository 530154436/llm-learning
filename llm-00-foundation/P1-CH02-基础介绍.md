## 第二章 基础介绍

大语言模型是指在海量无标注文本数据上进行预训练得到的大型预训练语言模型，例如 `GPT-3`、`PaLM`和`LLaMA`。通常是指参数规模达到百亿、千亿甚至万亿的模型；本书泛指具有超大规模参数或者经过超大规模数据训练所得到的语言模型。

### 2.1 大语言模型的构建过程

从机器学习的观点来说，大语言模型是一种基于 Transformer 结构的神经网络模型。因此，可以将大语言模型看作一种拥有大规模参数的函数，它的构建过程就是使用训练数据对于模型参数的拟合过程。大语言模型的优化目标更加泛化，不仅仅是为了解决某一种或者某一类特定任务，而是希望能够作为通用任务的求解器。一般来说，大语言模型的训练过程可以分为`大规模预训练`和`指令微调与人类对齐`两个阶段。

#### 2.1.1 大规模预训练

一般来说，`预训练`是指用与下游任务无关的大规模数据进行模型参数的初始训练。 OpenAI 前首席科学家 Ilya Sutskever 在公开采访中指出大规模预训练本质上是在做一个**世界知识的压缩**，从而能够学习到一个编码世界知识的参数模型，这个模型能够通过解压缩所需要的知识来解决真实世界的任务。

为了预训练大语言模型，需要准备大规模的文本数据，并且进行严格的清洗，去除掉可能包含有毒有害的内容，最后将清洗后的数据进行词元化（Tokenization）流，并且切分成批次（Batch），用于大语言模型的预训练。 由于大语言模型的能力基础主要来源于预训练数据，因此收集高质量、多源化的数据以及对于数据进行严格的清洗对于模型性能具有重要的影响。

目前的开源模型普遍采用 2∼3T 规模的词元进行预训练，这一过程对于算力需求量极高，一般来说训练百亿模型至少需要百卡规模的算力集群（如 A100 80G）联合训练数月时间（与具体的算力资源相关）；而训练千亿模型则需要千卡 甚至万卡规模的算力集群，对于算力资源的消耗非常惊人。 尽管整体的预训练技术框架非常直观，但是实施过程中涉及到大量需要深入探索的经验性技术，如`数据如何进行配比`、`如何进行学习率的调整`、`如何早期发现模型的异常行为`等。

#### 2.1.2 指令微调与人类对齐

经过大规模数据预训练后的语言模型已经具备较强的模型能力，能够编码丰富的世界知识，但是由于预训练任务形式所限，这些模型更擅长于文本补全，并不适合直接解决具体的任务。尽管可以通过上下文学习（In-Context Learning, ICL） 等提示学习技术进行适配，但是模型自身对于任务的感知与解决能力仍然较为局限。

目前来说，比较广泛使用的微调技术是“`指令微调`”（也叫做有监督微调，Supervised Fine-tuning, SFT），通过使用任务输入与输出的配对数据进行模型训练，可以使得语言模型较好地掌握通过`问答形式`进行任务求解的能力。 一般来说，指令微调很难教会大语言模型预训练阶段没有学习到的知识与能力，它主要起到了`对于模型能力的激发作用，而不是知识注入作用`。 通常来说，数十万到百万规模的指令微调数据能够有效地激发语言模型的通用任务解决能力，甚至有些工作认为`数千条`或者数万条高质量指令数据也能达到不错的微调效果。因此，指令微调对于算力资源的需求相对较小。一般情况下，若干台单机八卡（A100-80G）的服务器就能在一天或数天的时间内完成百亿模型的指令微调。

除了提升任务的解决能力外，还需要将大语言模型与人类的期望、需求以及价值观对齐（Alignment），这对于大模型的部署与应用具有重要的意义。OpenAI 在 2022 年初发布了 `InstructGPT`的学术论文，系统地介绍了如何将语言模型进行人类对齐。具体来说，主要引入了基于人类反馈的强化学习对齐方法 `RLHF`（Reinforcement Learning from Human Feedback），在指令微调后使用强化学习加强模型的对齐能力。 在 RLHF 算法中，需要训练一个符合人类价值观的奖励模型（Reward Model）。为此，需要标注人员针对大语言模型所生成的多条输出进行偏好排序，并使用偏好数据训练奖励模型，用于判断模型的输出质量。由于强化学习需要维护更多的辅助模型进行训练，通常来说对于资源的消耗会多于指令微调，但是也远小于预训练阶段所需要的算力资源。

经历上述两个过程后，大语言模型就能够具备较好的人机交互能力，通过问答形式解决人类所提出的问题。


### 2.2 扩展法则

在实现上，大语言模型采用了与小型预训练语言模型相似的神经网络结构（基于注意力机制的 Transformer 架构）和预训练方法（如语言建模）。但是通过扩展参数规模、数据规模和计算算力，大语言模型的能力显著超越了小型语言模型的能力。因此，建立定量的建模方法，即`扩展法则`（Scaling Law），来研究规模扩展所带来的模型性能提升具有重要的实践指导意义。

#### 2.2.1 KM扩展法则
2020 年，OpenAI 团队首次建立了神经语言模型性能与三个主要因素——`模型规模`（𝑁）、`数据规模`（𝐷）和`计算算力`（𝐶）之间的幂律关系（Power-Law Relationship）。在给定算力预算 $c$ 的条件下，可以近似得到以下三个基本指数公式 2.1 来描述扩展法则：
 
$$
L(N) = \left(\frac{N_c}{N}\right)^{\alpha_N}, \quad \alpha_N \sim 0.076, \quad N_c \sim 8.8 \times 10^{13} 
$$
 
$$
L(D) = \left(\frac{D_c}{D}\right)^{\alpha_D}, \quad \alpha_D \sim 0.095, \quad D_c \sim 5.4 \times 10^{13} 
$$
 
$$
L(C) = \left(\frac{C_c}{C}\right)^{\alpha_C}, \quad \alpha_C \sim 0.050, \quad C_c \sim 3.1 \times 10^8
$$

其中，<br> 
$L(\cdot)$ 表示以 $nat$ 为单位的交叉熵损失，衡量模型性能（损失越小，性能越好）。<br>
$N_c$ 、 $D_c$ 和 $C_c$ 分别表示非嵌入参数数量、训练数据数量和实际的算力开销的临界值，超过这些值后，模型性能的提升会显著放缓。

为了推导这些公式，需要约定一些基本假设：一个因素的分析不会受到其他两个因素的限制，例如当变动模型参数规模的时候，需要保证数据资源是充足的。由公式 2.1 可见，模型性能与这三个因素之间存在着较强的依赖关系，可以近似刻画为指数关系。 为了便于理解扩展法则对于模型性能的影响，将这里的损失函数进一步分解为两部分，包括不可约损失（**真实数据分布的熵**）和可约损失（**真实分布和模型分布之间 KL 散度的估计**）：
 
$$
L(x) = L_\infty \quad \text{(不可约损失)} + \left(\frac{x_0}{x}\right)^{\alpha_x} \quad \text{(可约损失)}
$$
 
其中， $x$ 是一个占位符号，可以指代公式 2.1 中的 $N$ 、 $D$ 和 $C$ 。<br>
不可约损失 由数据自身特征确定，是模型性能的理论下限，无法通过增加模型规模、数据规模或计算算力来减少。<br>
可约损失：可以通过增加（$N$ 、 $D$ 和 $C$）来减少，从而提升模型性能，模型性能的优化只能减小可约损失部分。


#### 2.2.2 Chinchilla 扩展法则  
Hoffmann 等人（DeepMind 团队）于2022年提出了一种可选的扩展法则，旨在指导大语言模型在给定算力资源下优化训练。通过针对更大范围的模型规模（70M 到 16B 参数）和数据规模（5B 到 500B 词元）进行实验，研究人员拟合得到了另一种关于模型性能的幂律关系：
 
$$  
L(N,D) = E + \frac{A}{N^\alpha} + \frac{B}{D^{\beta}}  
$$  
 
其中， $E=1.69$ ， $A=406.4$ ， $B=410.7$ ， $\alpha=0.34$ ， $\beta=0.28$ 。进一步利用约束条件 $C \approx 6ND$ 对损失函数 $L(N,D)$ 进行推导（朗格朗日乘子法），可得到算力资源固定时模型规模与数据规模的最优分配方案：
 
$$
N_{\text{opt}}(C) = G \left( \frac{C}{6} \right)^a, \quad D_{\text{opt}}(C) = G^{-1} \left( \frac{C}{6} \right)^b
$$  
 
这里， $a = \frac{\alpha}{\alpha + \beta}$ ， $b = \frac{\beta}{\alpha + \beta}$ ， $G$ 是由 $A$ 、 $B$ 、 $\alpha$ 和 $\beta$ 计算得出的扩展系数。
 
KM 扩展法则和 Chinchilla 扩展法则均可近似表示为以算力为核心的公式（公式 2.4）：  
 
$$  
N_{\text{opt}} \approx C^a, \quad D_{\text{opt}} \approx C^b  
$$  
 
即当算力C给定的情况下，最优的模型参数规模和数据规模由指数系数 $a$ 和 $b$ 分别确定。 随着算力预算的增加，KM扩展法则倾向于将更大的预算分配给模型规模的增加，而不是分配给数据规模的增加；而Chinchilla扩展法则主张两种规模参数应该以等比例关系增加。 Chinchilla 扩展法则这项研究的意义并不在于给出了资源在数据规模与模型规模上的具体分配方案，而是首次形式化指出了之前的预训练工作可能忽视了训练数据的规模扩展。

越来越多的工作表明，现有的预训练语言模型对于数据的需求量远高于这些扩展法则中所给出的估计规模。这种现象的一个重要原因是由于 Transformer 架构具有较好的数据扩展性，到目前为止，还没有实验能够有效验证特定参数规模语言模型的饱和数据规模（即随着数据规模的扩展，模型性能不再提升）。

#### 2.2.3 关于扩展法则的讨论

+ **可预测的扩展**（Predictable Scaling）<br>
在实践中，扩展法则可以用于指导大语言模型的训练，通过较小算力资源可靠地估计较大算力资源投入后的模型性能，这被称为`可预测的扩展`。这种可预测性主要体现在两个方面：使用小模型的性能去预估大模型的性能，或者使用大模型的早期训练性能去估计训练完成后的性能。 基于小模型获得训练经验然后应用于大模型的训练，从而减少实验成本。其次，大语言模型的训练过程较长，经常面临着训练损失波动情况，扩展法则可以用于监控大语言模型的训练状态，如在早期识别异常性能。

+ **任务层面的可预测性**<br>.
整体上来说，语言建模损失较小的模型往往在下游任务中表现更好，因为语言建模的能力可以被认为是一种模型整体能力的综合度量。然而，语言建模损失的减少并不总是意味着模型在下游任务上的性能改善。对于某些特殊任务，甚至会出现“逆向扩展”（Inverse Scaling）现象，即随着语言建模损失的降低，任务性能却出人意料地变差。


### 2.3 涌现能力

大语言模型的`涌现能力`被非形式化定义为“在小型模型中不存在但在大模型中出现的能力”，具体是指当模型扩展到一定规模时，模型的特定任务性能突然出现显著跃升的趋势，远超过随机水平。

#### 2.3.1 代表性的涌现能力

+ **上下文学习**（In-context Learning, ICL）<br>
上下文学习能力在GPT-3的论文中被正式提出。具体方式为，在提示中为语言模型提供`自然语言指令`和`多个任务示例`（Demonstration），无需显式的训练或梯度更新，仅输入文本的单词序列就能为测试样本生成预期的输出。

+ **指令遵循**（Instruction Following）<br>
指令遵循能力是指大语言模型能够按照自然语言指令来执行对应的任务。为了获得这一能力，通常需要使用自然语言描述的多任务示例数据集进行微调，称为指令微调（Instruction Tuning）或监督微调（Supervised Fine-tuning）。通过指令微调，大语言模型可以在没有使用显式示例的情况下按照任务指令完成新任务，有效提升了模型的泛化能力。 相比于上下文学习能力，指令遵循能力整体上更容易获得，但是最终的任务执行效果还取决于模型性能和任务难度决定。

+ **逐步推理**（Step-by-step Reasoning）<br>
对于小型语言模型而言，通常很难解决涉及多个推理步骤的复杂任务（如数学应用题），而大语言模型则可以利用`思维链`（Chain-of-Thought, CoT）提示策略 [25] 来加强推理性能。具体来说，大语言模型可以在提示中引入任务相关的中间推理步骤来加强复杂任务的求解，从而获得更为可靠的答案。

通常来说，很难统一界定大语言模型出现这些上述能力的临界规模（即具备某种能力的最小规模），因为能力涌现会受到多种因素或者任务设置的影响。最近的研究表明，经过了高质量的预训练与指令微调后，即使较小的语言模型（如
LLaMA-2 (7B)）也能够一定程度上展现出上述提到的三种能力，并且对于参数规模的要求随着预训练数据规模的扩展以及数据质量的提升在不断下降。

#### 2.3.2 涌现能力与扩展法则的关系
扩展法则使用语言建模损失来衡量语言模型的整体性能，整体上展现出了较为平滑的性能提升趋势，具有较好的可预测性，但是指数形式暗示着可能存在的边际效益递减现象；而涌现能力通常使用任务性 能来衡量模型性能，整体上展现出随规模扩展的骤然跃升趋势，不具有可预测性， 但是一旦出现涌现能力则意味着模型性能将会产生大幅跃升。

目前还缺少对于大语言模型涌现机理的基础性解释研究工作。

### 2.4 GPT系列模型的技术演变

2022 年 11 月底，OpenAI 推出了基于大语言模型的在线对话应用—ChatGPT。 由于具备出色的人机对话能力和任务解决能力，ChatGPT 一经发布就引发了全社会对于大语言模型的广泛关注，众多的大语言模型应运而生，并且数量还在不断增加（图 2.1）。

<img src="images/图 2.1 大语言模型发展时间线.png" width="70%" height="70%" alt="">

GPT 系列模型的基本原理是训练模型学习恢复预训练文本数据，将广泛的世界知识压缩到仅包含解码器（Decoder-Only）的 Transformer 模型中，从而使模型 能够学习获得较为全面的能力。其中，两个关键要素是：（I）训练能够准确预测下一个词的 Transformer （只包含解码器）语言模型；（II）扩展语言模型的规模以及扩展预训练数据的规模。截止到目前，OpenAI 对大语言模型的研发历程大致可分为四个阶段：早期探索阶段、路线确立阶段、能力增强阶段以及能力跃升阶段。

<img src="images/图 2.2 GPT 系列模型技术发展的历程图.png" width="70%" height="70%" alt="">


#### 2.4.1 早期探索阶段
**GPT-1**：基于生成式、仅有解码器的Transformer架构开发，奠定了GPT系列模型的核心架构与基于自然语言文本的预训练方式。

**GPT-2**：GPT-2旨在探索通过扩大模型参数规模来提升模型性能，并且尝试去除针对特定任务所需要的微调环节。它试图使用无监督预训练的语言模 型来解决各种下游任务，进而不需要使用标注数据进行显式的模型微调。 为了建立通用的多任务学习框架，GPT 系列模型将输入、输出和任务信息都通过自然语言形式进行描述，进而后续任务的求解过程就可以看作是任务方案（或答案）的文本生成问题。语言模型将每个（自然语言处理）任务都视为基于世界文本子集的下一个词预测问题。因此，如果无监督语言建模经过训练后具有足够的能力复原全部世界文本，那么本质上它就能够解决各种任务。

#### 2.4.2 路线确立阶段
**GPT-3**：与 GPT-2 相比，GPT-3 直接将参数规模提升了 100 余倍，并且提出了“上下文学习”这一概念，使得大语言模型可以通过少样本学习的方式来解决各种任务。GPT-3可以被看作从预训练语言模型到大语言模型演进过程中的一个重要里程碑，它证明了将神经网络扩展到超大规模可以带来大幅的模型性能提升，并且建立了以`提示学习方法`为基础技术路线的任务求解范式。

#### 2.4.3 能力增强阶段
根据公开资料披露的内容来说，OpenAI 探索了两种主要途径来改进GPT-3 模型，即代码数据训练和人类偏好对齐。

+ `代码数据训练`<br>
原始的 GPT-3 模型的复杂推理任务能力仍然较弱，如对于编程问题和数学问题的求解效果不好。
为了解决这一问题，OpenAI 于 2021 年 7 月推出了 `Codex`，这是一个在大量 GitHub 代码数据集合上微调的 GPT 模型，不仅可以解决非常困难的编程问题，还能显著提升大模型解决数学问题的能力。

+ `人类偏好对齐`<br>
2022 年 1 月，OpenAI 正式推出 `InstructGPT`这一具有重要影响力的学术工作，旨在改进 GPT-3 模型与人类对齐的能力，正式建立了基于人类反馈的强化学习算法，即 RLHF 算法。通过这些增强技术，OpenAI 将改进后的具有更强能力的 GPT 模型命名为 `GPT-3.5` 模型。

#### 2.4.4 能力跃升阶段

**ChatGPT**：将人类生成的对话数据（同时扮演用户和人工智能的角色）与训练 InstructGPT 的相关数据进行结合，并统一成对话形式用于训练 ChatGPT。ChatGPT 在与人机对
话测试中展现出了众多的优秀能力：拥有丰富的世界知识、复杂问题的求解能力、 多轮对话的上下文追踪与建模能力、与人类价值观对齐的能力等。

**GPT-4**：继 ChatGPT 后，OpenAI 于 2023 年 3 月发布了 GPT-4，它首次将GPT 系列模型的输入由单一文本模态扩展到了图文双模态。 OpenAI 强调了安全开发 GPT-4 的重要性，并应用了一些干预策略来缓解大语言模型可能出现的问题——幻觉、隐私泄露等。更重要的是，GPT-4 搭建了完备的深度学习训练基础架构，进一步引入了可预测扩展的训练机制，可以在模型训练过程中通过较少计算开销来准确预测模型的最终性能。


**GPT-4V**、**GPT-4 Turbo**以及多模态支持模型：GPT-4V 在多种应用场景中表现出了强大的视觉能力与综合任务解决能力。 GPT-4 Turbo，引入了一系列技术升级：提升了模型的整体能力（比 GPT-4 更强大），扩展了知识来源（拓展到 2023 年 4 月），支持更长上下文窗口（达到 128K），优化了模型性能（价格更便宜），引入了若干新的功能（如函数调用、可重复输出等）。 同时，`Assistants API` 功能也被推出，旨在提升人工智能应用助手的开发效率，开发人员可以利用特定的指令、外部知识和工具，在应用程序中快速创建面向特定任务目标的智能助手。

尽管 GPT 系列模型取得了巨大的科研进展，这些最前沿的大语言模型仍然存在一定的局限性。例如，GPT 模型可能在某些特定上下文中生成带有事实错误的内容（即幻觉）或存在潜在风险的回应。
