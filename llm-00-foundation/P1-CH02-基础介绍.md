## 第二章 基础介绍

大语言模型是指在海量无标注文本数据上进行预训练得到的大型预训练语言模型，例如 `GPT-3`、`PaLM`和`LLaMA`。通常是指参数规模达到百亿、千亿甚至万亿的模型；本书泛指具有超大规模参数或者经过超大规模数据训练所得到的语言模型。

### 2.1 大语言模型的构建过程

从机器学习的观点来说，大语言模型是一种基于 Transformer 结构的神经网络模型。因此，可以将大语言模型看作一种拥有大规模参数的函数，它的构建过程就是使用训练数据对于模型参数的拟合过程。大语言模型的优化目标更加泛化，不仅仅是为了解决某一种或者某一类特定任务，而是希望能够作为通用任务的求解器。一般来说，大语言模型的训练过程可以分为`大规模预训练`和`指令微调与人类对齐`两个阶段。

#### 2.1.1 大规模预训练

一般来说，`预训练`是指<u>使用与下游任务无关的大规模数据进行模型参数的初始训练</u>。 OpenAI 前首席科学家 Ilya Sutskever 在公开采访中指出大规模预训练本质上是在做一个<u>世界知识的压缩</u>，从而能够学习到一个编码世界知识的参数模型，这个模型能够通过解压缩所需要的知识来解决真实世界的任务。

为了预训练大语言模型，需要准备大规模的文本数据，并且进行严格的清洗，去除掉可能包含有毒有害的内容，最后将清洗后的数据进行词元化（Tokenization）流，并且切分成批次（Batch），用于大语言模型的预训练。 由于大语言模型的能力基础主要来源于预训练数据，因此收集高质量、多源化的数据以及对于数据进行严格的清洗对于模型性能具有重要的影响。

目前的开源模型普遍采用 2∼3T 规模的词元进行预训练，这一过程对于算力需求量极高，一般来说训练百亿模型至少需要百卡规模的算力集群（如 A100 80G）联合训练数月时间（与具体的算力资源相关）；而训练千亿模型则需要千卡 甚至万卡规模的算力集群，对于算力资源的消耗非常惊人。 尽管整体的预训练技术框架非常直观，但是实施过程中涉及到大量需要深入探索的经验性技术，如`数据如何进行配比`、`如何进行学习率的调整`、`如何早期发现模型的异常行为`等。

#### 2.1.2 指令微调与人类对齐

经过大规模数据预训练后的语言模型已经具备较强的模型能力，能够编码丰富的世界知识，但是由于预训练任务形式所限，这些模型更擅长于文本补全，并不适合直接解决具体的任务。尽管可以通过上下文学习（In-Context Learning, ICL） 等提示学习技术进行适配，但是模型自身对于任务的感知与解决能力仍然较为局限。

目前来说，比较广泛使用的微调技术是“`指令微调`”（也叫做有监督微调，Supervised Fine-tuning, SFT），通过使用任务输入与输出的配对数据进行模型训练，可以使得语言模型较好地掌握通过<u>问答形式</u>进行任务求解的能力。 一般来说，指令微调很难教会大语言模型预训练阶段没有学习到的知识与能力，它主要<u>起到了对于模型能力的激发作用，而不是知识注入作用</u>。 通常来说，数十万到百万规模的指令微调数据能够有效地激发语言模型的通用任务解决能力，甚至有些工作认为`数千条`或者数万条高质量指令数据也能达到不错的微调效果。因此，指令微调对于算力资源的需求相对较小。一般情况下，若干台单机八卡（A100-80G）的服务器就能在一天或数天的时间内完成百亿模型的指令微调。

除了提升任务的解决能力外，还需要将大语言模型与人类的期望、需求以及价值观对齐（Alignment），这对于大模型的部署与应用具有重要的意义。OpenAI 在 2022 年初发布了 `InstructGPT`的学术论文，系统地介绍了如何将语言模型进行人类对齐。具体来说，主要引入了基于人类反馈的强化学习对齐方法 `RLHF`（Reinforcement Learning from Human Feedback），在指令微调后使用强化学习加强模型的对齐能力。 在 RLHF 算法中，需要训练一个符合人类价值观的奖励模型（Reward Model）。为此，需要标注人员针对大语言模型所生成的多条输出进行偏好排序，并使用偏好数据训练奖励模型，用于判断模型的输出质量。由于强化学习需要维护更多的辅助模型进行训练，通常来说对于资源的消耗会多于指令微调，但是也远小于预训练阶段所需要的算力资源。

经历上述两个过程后，大语言模型就能够具备较好的人机交互能力，通过问答形式解决人类所提出的问题。


### 2.2 扩展法则

在实现上，大语言模型采用了与小型预训练语言模型相似的神经网络结构（基于注意力机制的 Transformer 架构）和预训练方法（如语言建模）。但是通过扩展参数规模、数据规模和计算算力，大语言模型的能力显著超越了小型语言模型的能力。因此，建立定量的建模方法，即`扩展法则`（Scaling Law），来研究规模扩展所带来的模型性能提升具有重要的实践指导意义。

#### 2.2.1 KM扩展法则
2020 年，OpenAI 团队首次建立了神经语言模型性能与三个主要因素——模型规模（𝑁）、数据规模（𝐷）和计算算力（𝐶）之间的幂律关系（Power-Law Relationship）。在给定算力预算 $c$ 的条件下，可以近似得到以下三个基本指数公式来描述扩展法则：

$$
L(N)=\left(\frac{N_c}{N}\right)^{\alpha_N},\alpha_N\sim0.076,N_c\sim8.8\times10^{13} 
\\ L(D)=\left(\frac{D_c}{D}\right)^{\alpha_D},\alpha_D\sim0.095,D_c\sim5.4\times10^{13} 
\\ L(C)=\left(\frac{C_c}{C}\right)^{\alpha_C},\alpha_C\sim0.050,C_c\sim3.1\times10^8 \tag{2.1}
$$

这里，$L(\cdot)$ 表示用以 $nat$ 为单位的交叉熵损失。其中，$N_c$ 、$D_c$ 和 $C_c$ 分别表示非嵌入参数数量、训练数据数量和实际的算力开销。
为了推导这些公式，需要约定一些基本假设：一个因素的分析不会受到其他两个因素的限制，如当变动模型参数规模的时候，需要保证数据资源是充足的。

由公式2.1可见，模型性能与这三个因素之间存在着较强的依赖关系，可以近似刻画为指数关系。

为了便于理解扩展法则对于模型性能的影响，将这里的损失函数进一步分解为两部分，包括不可约损失（真实数据分布的熵）和可约损失（真实分布和模型分布之间KL散度的估计）：

$$
L(x)=\underbrace{L_\infty}_{不可约损失}+ \underbrace{\left(\frac{x_0}{x}\right)^{\alpha_x}}_{可约损失} \\
$$

这里 $x$ 是一个占位符号，可以指代公式2.1中的 $N$、$D$ 和 $C$。其中，不可约损失由数据自身特征确定，无法通过扩展法则或者优化算法进行约减；模型性能的优化只能减小可约损失部分。






















