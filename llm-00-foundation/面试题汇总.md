## 面试题
### 一、大模型
#### 1.1 基础知识
1. Transformer及其变种
解释一下Transformers的结构？<br>
Transformer与传统的transformer模型在架构、归一化层、激活函数等方面有哪些不同？<br>
InstructGPT的三个阶段分别是怎样的？<br>
Flash-attention的工作原理是什么？<br>
2. 注意力机制<br>
多头注意力机制和分组注意力机制有什么区别？<br>
Lora的原理是什么？参数r和alpha分别代表什么意义？<br>
3. 位置编码<br>
相对位置编码和绝对位置编码的区别是什么？<br>
旋转位置编码是如何工作的？<br>
4. 其他重要概念<br>
LayerNorm在模型中的作用是什么？<br>
Deepspeed的Zero1、Zero2和Zero3之间有什么主要区别？<br>
DPO、PPO和GRPO之间的主要区别是什么？<br>
AdamW相对于Adam有哪些优化？<br>
5. 强化学习与反馈系统<br>
强化学习人类反馈（RLHF）的原理是什么？它的主要阶段有哪些？<br>
SFT和RLHF分别适用于哪些场景？<br>
PPO算法的基本原理是什么？<br>

6. 提示词工程<br>
解释一下提示词工程中的temperature、top_p、top_k、repeat等参数的意义。<br>
7. 缓存机制<br>
kv_cache的工作原理是什么？<br>
#### 1.2 项目经验
1. 微调及应用<br>
描述一个你使用BERT进行微调的项目。<br>
你有没有做过LLaVA模型的分类微调？如果有，请简要说明。<br>
工单处理中使用的大模型的作用是什么？<br>
如何处理多轮对话中的Query改写数据？<br>
病例描述的评价机制是什么？<br>
2. 数据处理与优化<br>
在你的项目中，是如何处理文档和图表的？<br>
如何优化以避免灾难性遗忘？<br>
微调后的badcase你是如何处理的？<br>
如何评估基于Qwen0.5b做的embedding与BGE的embedding检索效果？<br>
3. 向量数据库与加速技术<br>
向量数据库的作用是什么？如何利用它来加速模型离线时的向量化？<br>
你如何实现vllm大模型的加速？<br>

4. 其他<br>
大模型训练的一般流程是怎样的？<br>
RAG专家问答系统的整体流程是什么？<br>
智能问答系统（RAG）的整体流程是怎样的？<br>
请分享一个病历生成项目的多任务SFT过程。<br>

awesome_LLMs_interview_notes
https://github.com/naginoa/LLMs_interview_notes/tree/main