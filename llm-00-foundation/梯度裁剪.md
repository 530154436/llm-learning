# 权重衰减（Weight Decay）

## 1. 作用与背景
权重衰减是一种正则化技术（等价于L2正则化），通过惩罚大权重值防止模型过拟合。然而，某些参数（如偏置项、归一化层的参数）通常不需要应用权重衰减，因为它们对模型的正则化效果贡献较小，甚至可能损害训练稳定性。

## 2. 参数类型解析

| 参数名称             | 参数类型      | 示例模块                 |
|------------------|-----------|----------------------|
| bias             | 偏置项       | nn.Linear, nn.Conv2d |
| LayerNorm.bias   | 层归一化的偏置参数 | nn.LayerNorm         |
| LayerNorm.weight | 层归一化的缩放参数 | nn.LayerNorm         |

### 为什么这些参数不需要权重衰减？
- **偏置项（bias）**：
  偏置参数通常用于调整神经元的激活阈值，其数值大小对模型的表达能力影响较小，正则化反而可能抑制灵活性。
  
- **归一化层参数（LayerNorm）**：
  层归一化的 weight（缩放参数）和 bias（偏移参数）本质上是学习数据分布的均值和方差，对其进行正则化会破坏归一化的统计特性，可能导致训练不稳定。

## 3. 实际应用场景
在优化器（如AdamW）的设置中，通常会将模型的参数分为两组：
- 应用权重衰减的参数：如权重矩阵（weight）。
- 不应用权重衰减的参数：即 no_decay 列表中的参数。

### 代码示例
```python
# 定义参数分组规则
optimizer_grouped_parameters = [
    # 第一组：需要权重衰减的参数（排除 no_decay 列表中的参数）
    {
        'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
        'weight_decay': 0.01
    },
    # 第二组：不需要权重衰减的参数（no_decay 列表中的参数）
    {
        'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
        'weight_decay': 0.0
    }
]

# 初始化优化器
optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5)
```

## 4. 技术细节与注意事项
(1) 权重衰减的影响
权重参数（weight）： 正则化可防止权重过大，增强泛化能力。
归一化层参数： 若错误地应用权重衰减，可能破坏归一化后的数据分布（如缩放参数被压缩），导致梯度异常。
(2) 通用性
适用任务： 文本分类、机器翻译、预训练模型微调（如BERT微调）。
适用框架： PyTorch、TensorFlow（需调整参数名称匹配逻辑）。
## 5. 扩展：其他归一化层的处理
如果模型中包含其他类型的归一化层，需扩展 no_decay 列表：
批量归一化（BatchNorm）：
```python
no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight', 'BatchNorm.bias', 'BatchNorm.weight']
```
实例归一化（InstanceNorm）：
```
no_decay = ['bias', 'InstanceNorm.bias', 'InstanceNorm.weight']
```
总结：
需要标识出模型中不需要应用权重衰减的参数（偏置项、归一化层参数），在优化器设置中为其分配 weight_decay=0.0。
这种设计是深度学习模型训练的常见最佳实践，尤其适用于Transformer架构的模型（如BERT），能够提升训练稳定性与最终性能。


Bert在fine-tune时训练的5种技巧
https://zhuanlan.zhihu.com/p/524036087

不需要应用权重衰减（Weight Decay）的参数
https://blog.csdn.net/weixin_44012667/article/details/146826174
