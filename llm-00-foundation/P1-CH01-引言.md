<nav>
<a href="#第一章-引言">第一章 引言</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#11-语言模型的发展历程">1.1 语言模型的发展历程</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#12-大语言模型的能力特点">1.2 大语言模型的能力特点</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#13-大语言模型关键技术">1.3 大语言模型关键技术</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#14-大语言模型对科技发展的影响">1.4 大语言模型对科技发展的影响 </a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#参考引用">参考引用</a><br/>
</nav>


## 第一章 引言

从技术路径上来说，语言模型（Language Model，LM）是提升机器语言智能（Language Intelligence）的主要技术途径之一。

### 1.1 语言模型的发展历程

语言模型旨在对人类语言的内在规律进行建模，预测词序列中未来（或缺失）词或词元（Token）的概率。其发展经历了四个主要阶段：
 
- **统计语言模型**（Statistical Language Model, SLM）<br>
20世纪90年代兴起，基于统计学习方法，使用马尔可夫假设（Markov Assumption）来建立语言序列的预测模型，通常是根据词序列中若干个连续的上下文单词来预测下一个词的出现概率，即根据一个固定长度的前缀来预测目标单词。
常见形式为`n-gram模型`（具有固定上下文长度 𝑛 的统计语言模型，如二元、三元模型），被广泛应用于信息检索（Information Retrieval, IR）和自然语言处理（Natural Language Processing, NLP）等领域的早期研究工作。<br>
缺点：随着阶数 𝑛 的增加，高阶模型面临“维数灾难”（Curse of Dimensionality）；数据稀疏问题严重；平滑策略效果有限。<br>

- **神经语言模型**  （Neural Language Model, NLM）<br>
使用神经网络来建模文本序列的生成，如循环神经网络（Recurrent Neural Networks, RNN）. Yoshua Bengio引入了分布式词表示（Distributed Word Representation）这一概念，并构建了基于聚合上下文特征（即分布式词向量）的目标词预测函数。 分布式词表示使用低维稠密向量来表示词汇的语义，能够刻画更为丰富的隐含语义特征，有效克服统计语言模型中的数据稀疏问题。分布式词向量又称为“词嵌入”（Word Embedding），`word2vec`是具有代表性的词嵌入学习模型，在自然语言处理任务中得到了广泛使用，取得了显著的性能提升。
缺点：word2vec在训练后每个单词的表示是固定的，不随上下文变化；无法区分多义词的不同语义，例如“bank”在不同上下文中可能指“河岸”或“银行”。 

- **预训练语言模型**（Pre-trained Language Model, PLM）<br>
2017年，谷歌提出了基于自注意力机制（Self-Attention）的`Transformer`模型，该模型能够有效建模长程序列关系，并且对硬件友好，支持通过GPU或TPU加速训练。
基于Transformer架构，谷歌推出了预训练语言模型`BERT`，它使用了仅有编码器的Transformer架构；与此同时，OpenAI也采用了Transformer架构来训练`GPT-1`，与BERT不同，GPT-1使用仅有解码器的Transformer架构，并通过预测下一个词元进行预训练。
一般来说，编码器架构被认为更适合去解决自然语言理解任务（如完形填空等），而解码器架构更适合解决自然语言生成任务（如文本摘要等）。
以 BERT、GPT-1 为代表的预训练语言模型确立了`“预训练-微调”`这一任务求解范式。其中，预训练阶段旨在通过大规模无标注文本建立模型的基础能力，而微调阶段则使用有标注数据对于模型进行特定任务的适配，从而更好地解决下游的自然语言处理任务。

- **大语言模型** （Large Language Model, LLM）<br>
研究人员发现，通过规模扩展（如增加模型参数规模或数据规模）通常会带来下游任务的模型性能提升，这种现象通常被称为“`扩展法则`”（Scaling Law）。
大模型具有但小模型不具有的能力通常被称为“涌现能力”（Emergent Abilities）。为了区别这一能力上的差异，学术界将这些大型预训练语言模型命名为“大语言模型”（Large Language Model，LLM）。


<img src="images/ch01/图 1.2 基于任务求解能力的四代语言模型的演化过程.png" width="80%" height="80%" alt="">

早期的统计语言模型主要被用于（或辅助用于）解决一些特定任务，主要以信息检索、文本分类、语音识别等传统任务为主。随后，神经语言模型专注于学习任务无关的语义表征，旨在减少人类特征工程的工作量，可以大范围扩展语言模型可应用的任务。进一步，预训练语言模型加强了语义表征的上下文感知能力，并且可以通过下游任务进行微调，能够有效提升下游任务（主要局限于自然语言处理任务）的性能。随着模型参数、训练数据、计算算力的大规模扩展，最新一代大语言模型的任务求解能力有了显著提升，能够不再依靠下游任务数据的微调进行通用任务的求解。<br>

综上所述，在语言模型的演化过程中，可以解决的任务范围得到了极大扩展，所获得的任务性能得到了显著提高，这是人工智能历史上的一次重要进步。

### 1.2 大语言模型的能力特点
- **丰富的世界知识**：通过大规模文本数据预训练，学习到海量世界知识，远超传统模型。
- **通用任务解决能力**：通过下一个词预测任务，建立多任务学习能力，能够解决多样化的任务。
- **复杂任务推理能力**：在知识推理、数学计算等复杂任务中表现出色，展现了强大的综合推理能力。
- **人类指令遵循能力**：建立了自然语言形式的统一任务解决模式：任务输入与执行结果均通过自然语言（提示学习）进行表达。
- **人类对齐能力**：通过人类反馈强化学习（RLHF）等技术，确保模型输出符合人类价值观。
- **工具使用能力**：能够调用外部工具（如计算器、搜索引擎）扩展能力，解决非自然语言任务。
- 其他能力：长程对话语义一致性、新任务快速适配、人类行为模拟等。

### 1.3 大语言模型关键技术
- **规模扩展**<br>
2020年，OpenAI 从参数、数据、算力三个方面深入地研究了规模扩展对于模型性能所带来的影响，建立了定量的函数关系，称之为“`扩展法则`”（Scaling Law）。
2022年，谷歌子公司 DeepMind 发表了重要研究成果—`Chinchilla 扩展法则`。
早期的研究主要关注模型参数规模所带来的性能优势，最近的工作则是加大对于高质量数据的规模扩展。

- **数据工程**<br>
模型能力本质上是来源于所见过的训练数据。
目前来说，数据工程主要包括三个方面。
首先，需要对于数据进行全面的采集，拓宽高质量的数据来源；其次，需要对于收集到的数据进行精细的清洗，尽量提升用于大模型训练的数据质量；第三，需要设计有效的数据配比与数据课程，加强模型对于数据语义信息的利用效率。

- **高效预训练**<br>
成功训练出一个性能较强的大语言模型极具挑战性。
需要联合使用各种并行策略以及效率优化方法，包括 `3D并行`（数据并行、流水线并行、张量并行）、`ZeRO`（内存冗余消除技术）等。
为了有效支持分布式训练，很多研究机构发布了专用的分布式优化框架来简化并行算法的实现与部署，具有代表性的分布式训练软件包括 `DeepSpeed` 和 `Megatron-LM` ，它们能够有效支持千卡甚至万卡的联合训练。
由于大语言模型的训练需要耗费大量的算力资源，通常需要开展基于小模型的沙盒测试实验，进而确定面向大模型的最终训练策略。

- **能力激发**  
为了提升模型的任务求解能力，需要设计合适的指令微调以及提示策略进行激发或诱导。
通常来说，现有的研究认为指令微调无法向大模型注入新的知识，而是训练大模型学会利用自身所掌握的知识与信息进行任务的求解。
为此，研究人员提出了多种高级提示策略，包括`上下文学习`、`思维链提示`等，通过构建特殊的提示模板或者表述形式来提升大语言模型对于复杂任务的求解能力。

- **人类对齐**  
经过海量无标注文本预训练的大语言模型可能会生成有偏见、泄露隐私甚至对人类有害的内容。
在实践应用中，需要保证大语言模型能够较好地符合人类的价值观。
目前，比较具有代表性的对齐标准是“`3H对齐标准`”，即 Helpfulness（有用性）、Honesty（诚实性）和 Harmlessness（无害性）。
OpenAI 提出了基于人类反馈的强化学习算法（Reinforcement Learning from Human Feedback, `RLHF`），将人类偏好引入到大模型的对齐过程中：首先训练能够区分模型输出质量好坏的奖励模型，进而使用强化学习算法来指导语言模型输出行为的调整，让大语言模型能够生成符合人类预期的输出。最近学术界开始使用监督微调的对齐方式简化RLHF优化过程的算法，如`DPO算法`等。

- **工具使用**  
由于大语言模型的能力主要是通过大规模文本数据的语义学习所建立的，因此在非自然语言形式的任务（如数值计算）中能力较为受限。
通过让大语言模型学会使用各种工具的调用方式，进而利用合适的工具去实现特定的功能需求。例如，利用计算器进行精确的数值计算、利用搜索引擎检索最新的时效信息。
在技术路径上，工具调用能力主要是通过指令微调以及提示学习两种途径实现，而未经历过特殊训练或者缺乏有效提示的大语言模型则很难有效利用候选工具。


### 1.4 大语言模型对科技发展的影响 

大语言模型首次实现了单一模型可以有效解决众多复杂任务，人工智能算法从未如此强大。大语言模型对人工智能技术的未来发展方向带来了重要影响，下面以四个典
型的领域进行介绍：


- 自然语言处理：大语言模型可以作为一种通用的语言任务解决技术，能够通过特定的提示方式解决不同类型的任务，并且能够取得较为领先的效果。
- 信息检索：在基于大语言模型的信息系统中，人们可以通过自然语言对话的形式获得复杂问题的答案。例如微软的New Bing。
- 计算机视觉：由于开源大语言模型的出现，可以极大地简化多模态模型的实现难度，通过将图像、视频等模态的信息与文本语义空间相融合，可以通过计算量相对较少的微调方法来研发多模态大语言模型。例如 OpenAI 的 Sora 模型。
- 人工智能赋能的科学研究（AI4Science）：目前大语言模型技术已经广泛应用于数学、化学、物理、生物等多个领域，基于其强大的模型能力赋能科学研究。


此外，大语言模型对于产业应用带来了变革性的技术影响，将会催生一个基于大语言模型的应用生态系统。例如，微软 365（Microsoft 365）正利用大语言模型（即 Copilot）来加强自动化办公软件的自动化办公工作；OpenAI 也进一步推动Assistants API 和 GPTs 来推广大模型智能体（Agent）的研发，从而实现特定任务 的求解工具。

在未来，将出现更多的以大语言模型为基础技术架构的科技应用产品，简化原来繁复的功能处理流程，加快软件研发周期，极大地改善用户体验。

## 参考引用

[1] 赵鑫, 李军毅, 周昆, 唐天一, 和 文继荣. [大语言模型](https://llmbook-zh.github.io/). 高等教育出版社, 2024, 北京. <br>
[2] [NLP中的预训练技术发展史](https://caicaijason.github.io/2019/11/22/NLP%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/)<br>