## 第一章 引言

从技术路径上来说，语言模型（Language Model，LM）是提升机器语言智能（Language Intelligence）的主要技术途径之一。

### 1.1 语言模型的发展历程

语言模型旨在对人类语言的内在规律进行建模，预测词序列中未来（或缺失）词或词元（Token）的概率。其发展经历了四个主要阶段：

<img src="images/ch01/图 1.2 基于任务求解能力的四代语言模型的演化过程.png" width="50%" height="50%" alt="">

 
1. **统计语言模型**（Statistical Language Model, SLM）<br>
20世纪90年代兴起，基于统计学习方法，使用马尔可夫假设（Markov Assumption）来建立语言序列的预测模型，通常是根据词序列中若干个连续的上下文单词来预测下一个词的出现概率，即根据一个固定长度的前缀来预测目标单词。
常见形式为`n-gram模型`（具有固定上下文长度 𝑛 的统计语言模型，如二元、三元模型），被广泛应用于信息检索（Information Retrieval, IR）和自然语言处理（Natural Language Processing, NLP）等领域的早期研究工作。<br>
缺点：随着阶数 𝑛 的增加，高阶模型面临“维数灾难”（Curse of Dimensionality）；数据稀疏问题严重；平滑策略效果有限。<br>

2. **神经语言模型**  （Neural Language Model, NLM）<br>
使用神经网络来建模文本序列的生成，如循环神经网络（Recurrent Neural Networks, RNN）. Yoshua Bengio引入了分布式词表示（Distributed Word Representation）这一概念，并构建了基于聚合上下文特征（即分布式词向量）的目标词预测函数。 分布式词表示使用低维稠密向量来表示词汇的语义，能够刻画更为丰富的隐含语义特征，有效克服统计语言模型中的数据稀疏问题。分布式词向量又称为“词嵌入”（Word Embedding），`word2vec`是具有代表性的词嵌入学习模型，在自然语言处理任务中得到了广泛使用，取得了显著的性能提升。
缺点：word2vec在训练后每个单词的表示是固定的，不随上下文变化；无法区分多义词的不同语义，例如“bank”在不同上下文中可能指“河岸”或“银行”。 

3. **预训练语言模型**（Pre-trained Language Model, PLM）<br>
2017年，谷歌提出了基于自注意力机制（Self-Attention）的`Transformer`模型，该模型能够有效建模长程序列关系，并且对硬件友好，支持通过GPU或TPU加速训练。
基于Transformer架构，谷歌推出了预训练语言模型`BERT`，它使用了仅有编码器的Transformer架构；与此同时，OpenAI也采用了Transformer架构来训练`GPT-1`，与BERT不同，GPT-1使用仅有解码器的Transformer架构，并通过预测下一个词元进行预训练。
一般来说，编码器架构被认为更适合去解决自然语言理解任务（如完形填空等），而解码器架构更适合解决自然语言生成任务（如文本摘要等）。
以 BERT、GPT-1 为代表的预训练语言模型确立了`“预训练-微调”`这一任务求解范式。其中，预训练阶段旨在通过大规模无标注文本建立模型的基础能力，而微调阶段则使用有标注数据对于模型进行特定任务的适配，从而更好地解决下游的自然语言处理任务。

4. **大语言模型** （Large Language Model, LLM）<br>
研究人员发现，通过规模扩展（如增加模型参数规模或数据规模）通常会带来下游任务的模型性能提升，这种现象通常被称为“`扩展法则`”（Scaling Law）。
大模型具有但小模型不具有的能力通常被称为“涌现能力”（Emergent Abilities）。为了区别这一能力上的差异，学术界将这些大型预训练语言模型命名为“大语言模型”（Large Language Model，LLM）。

早期的统计语言模型主要被用于（或辅助用于）解决一些特定任务，主要以信息检索、文本分类、语音识别等传统任务为主。
随后，神经语言模型专注于学习任务无关的语义表征，旨在减少人类特征工程的工作量，可以大范围扩展语言模型可应用的任务。
进一步，预训练语言模型加强了语义表征的上下文感知能力，并且可以通过下游任务进行微调，能够有效提升下游任务（主要局限于自然语言处理任务）的性能。
随着模型参数、训练数据、计算算力的大规模扩展，最新一代大语言模型的任务求解能力有了显著提升，能够不再依靠下游任务数据的微调进行通用任务的求解。
综上所述，在语言模型的演化过程中，可以解决的任务范围得到了极大扩展，所获得的任务性能得到了显著提高，这是人工智能历史上的一次重要进步。


> `ELMo`（Embeddings from Language Models）<br>
通过动态调整机制解决了词向量表示固定的问题，核心思想是在实际应用中根据单词的具体上下文调整其词嵌入，从而更准确地捕捉单词在特定语境下的意义。<br>
采用两阶段过程：首先使用大量的无标注数据训练双向LSTM网络学习上下文感知的单词表示，然后根据下游任务数据对 biLSTM 网络进行微调（Fine-Tuning），从而实现面向特定任务的模型优化。<br>
缺点：长文本建模能力较弱，并且不容易并行训练。<br>


## 1.2 大语言模型的能力特点
### 1.2.1 核心能力 
1. **丰富的世界知识**  
   - 通过大规模文本数据预训练，学习到海量世界知识，远超传统模型。
 
2. **通用任务解决能力**  
   - 通过下一个词预测任务，建立多任务学习能力，能够解决多样化的任务。
 
3. **复杂任务推理能力**  
   - 在知识推理、数学计算等复杂任务中表现出色，展现了强大的综合推理能力。
 
4. **人类指令遵循能力**  
   - 能够通过自然语言指令完成任务，为智能助手等应用提供了自然交互方式。
 
5. **人类对齐能力**  
   - 通过人类反馈强化学习（RLHF）等技术，确保模型输出符合人类价值观。
 
6. **工具使用能力**  
   - 能够调用外部工具（如计算器、搜索引擎）扩展能力，解决非自然语言任务。
 
### 1.2.2 其他能力 
- 长程对话语义一致性、新任务快速适配、人类行为模拟等。
 
---
 
## 1.3 大语言模型关键技术概览 
 
### 1.3.1 关键技术 
1. **规模扩展**  
   - 通过增加参数、数据、算力提升性能，如GPT-3的175B参数模型。
 
2. **数据工程**  
   - 数据采集、清洗、配比与课程设计，确保高质量训练数据。
 
3. **高效预训练**  
   - 使用分布式训练算法（如3D并行、ZeRO）优化大规模模型训练。
 
4. **能力激发**  
   - 通过指令微调、提示学习（如思维链提示）激发模型任务求解能力。
 
5. **人类对齐**  
   - 采用RLHF等技术，确保模型输出符合“3H标准”（有用性、诚实性、无害性）。
 
6. **工具使用**  
   - 通过插件机制调用外部工具，扩展模型能力范围。
 
---
 
## 1.4 大语言模型对科技发展的影响 
 
### 1.4.1 对人工智能的影响 
- **通用人工智能（AGI）**：大语言模型为实现AGI提供了新的可能性。
- **科研范式变革**：从特定任务优化到通用模型能力提升，研究重点发生转移。
 
### 1.4.2 对具体领域的影响 
1. **自然语言处理**  
   - 大语言模型成为通用语言任务解决技术，传统任务研究意义衰减。
 
2. **信息检索**  
   - 传统搜索引擎受到冲击，检索增强的大语言模型成为新兴方向。
 
3. **计算机视觉**  
   - 研发视觉-语言联合对话模型，推动多模态任务发展。
 
4. **AI4Science**  
   - 在数学、化学、生物等领域广泛应用，赋能科学研究。
 
### 1.4.3 对产业应用的影响 
- 催生基于大语言模型的应用生态系统，如微软365的Copilot、OpenAI的Assistants API。.

### 参考引用

[1] [NLP中的预训练技术发展史](https://caicaijason.github.io/2019/11/22/NLP%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/)<br>