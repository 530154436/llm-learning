<nav>
<a href="#第四章-数据准备">第四章 数据准备</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#41-数据来源">4.1 数据来源</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#411-通用文本数据">4.1.1 通用文本数据</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#412-通用文本数据">4.1.2 通用文本数据</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#42-数据预处理">4.2 数据预处理</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#421-质量过滤">4.2.1 质量过滤</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#基于启发式规则的方法">基于启发式规则的方法</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#基于分类器的方法">基于分类器的方法</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#422-敏感内容过滤">4.2.2 敏感内容过滤</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#423-数据去重">4.2.3 数据去重</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#424-数据对预训练效果的影响">4.2.4 数据对预训练效果的影响</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#数据数量的影响">数据数量的影响</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#数据质量的影响">数据质量的影响</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#数据集污染">数据集污染</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#43-词元化分词">4.3 词元化（分词）</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#431-bpe-分词">4.3.1 BPE 分词</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#432-wordpiece-分词">4.3.2 WordPiece 分词</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#433-unigram-分词">4.3.3 Unigram 分词</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#434-分词器的选用">4.3.4 分词器的选用</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#44-数据调度">4.4 数据调度</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#441-数据混合">4.4.1 数据混合</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#442-数据课程">4.4.2 数据课程</a><br/>
</nav>


## 第四章 数据准备

通过在大规模语料上进行`预训练`，大语言模型可以获得通用的语言理解与生成能力，掌握较为广泛的世界知识，具备解决众多下游任务的性能潜力。在这一过程中，预训练语料的规模和质量对于提升大语言模型的能力至关重要。<br>
预训练语料准备主要包含原始数据的收集、数据预处理、数据词元化、以及预训练过程中的数据调度方法。

### 4.1 数据来源

为了构建功能强大的大语言模型，需要从多元化的数据源中收集海量数据来进行训练。<br>
根据来源不同，预训练数据主要分为两种类型：通用文本数据和专用文本数据。`通用文本数据`涵盖了网页、书籍和对话文本等。由于通用文本数据规模较大、多样性强且易于获取，大多数大语言模型都会收集大量的通用文本数据，以增强其语言建模能力。此外，为了进一步提升大语言模型在特定专业任务上的表现，人们还将预训练语料的范围扩展至更`专业的数据集`，如多语数据、科学数据和代码数据等。

<img src="images/图 4.1 现有大语言模型预训练数据中各种数据来源的比例分布图.png" width="40%" height="40%" alt="">

#### 4.1.1 通用文本数据
+ **网页**<br>
使用大规模网页文本数据进行预训练，有助于大语言模型获取多样化的语言知识，并增强其自然语言理解和生成的能力。<br>
大规模网页数据集包括 C4、RefinedWeb、CC-Stories等。<br>
网页数据集中既包含了维基百科这种高质量文本，也不可避免地引入了广告网页等低质量文本。<br>
因此，在进行预训练之前，对网页进行筛选和处理显得尤为重要，这直接关系到最终数据的质量与预训练效果。<br>

+ **书籍**<br>
书籍文本在大语言模型的学习过程中，发挥着非常重要的作用，它们不仅能够帮助模型积累丰富的语言知识，还可以`加强其长程语义关系的建模`。
现有的研究工作通常使用 Books3 和 Bookcorpus2 等开源书籍数据集。<br>

#### 4.1.2 通用文本数据
+ **多语文本**<br>
在预训练语料中，加入多语言的文本数据可以增强模型的多语理解与生成能力。<br>
相比于仅针对单一目标语言进行微调的模型，在多语言语料库上训练过的大语言模型能够更好地建立`多语言间的语义关联`，为跨语言理解与对话任务提供支持。<br>
不仅如此，多语言数据还能有效增加数据的多样性，从而有助于提升模型的综合性能。

+ **科学文本**<br>
通过在大规模的科学文本语料上进行预训练，大语言模型可以在自然科学以及工程技术方面建立坚实的知识基础，从而在`科学问答与推理等任务`上取得出色的表现。<br>
构建科学文本语料的常用方法是收集 arXiv 论文、科学教材、数学网页等科学资源。<br>
然而，由于科学文本数据中包含数学公式、蛋白质序列等特殊符号，通常需要采用特定的分词和预处理技术，将这些不同格式的数据转化为大语言模型能够处理的统一格式。

+ **代码**<br>
为了提高模型的代码能力，需要在大量代码语料上进行预训练，进而提高其所生成的程序质量。<br>
一般来说，常用于大语言模型预训练的代码语料有两种来源，第一种是 Stack Exchange 等编程问答社区的数据，第二种是GitHub 等开源项目仓库。<br>
在代码数据上训练能够提升模型的`结构化语义理解`与`逻辑推理能力`。同时，代码中的函数调用关系还有助于增强模型的`工具使用与学习能力`。


### 4.2 数据预处理
当收集了丰富的文本数据之后，为了确保数据的质量和效用，还需要对数据进行`预处理`，从而消除低质量、冗余、无关甚可能有害的数据。（如开源库 `Data-Juicer`），

<img src="images/图 4.2 典型的预训练数据预处理流程图.png" width="60%" height="60%" alt="">


#### 4.2.1 质量过滤
为了优化模型学习的性能，需要去除语料库中的低质量数据。目前，研究人员主要使用以下两种数据清洗方法：（1）基于启发式规则的方法，和（2）基于分类器的方法。

##### 基于启发式规则的方法
+ `基于语种的过滤`<br>
为了训练特定目标语言为主导的大语言模型，通常要过滤掉其他语言的文本数据。<br>
需要注意的是，目前英文的高质量开放数据数量最多，已经成为了开源大语言模型的主要数据来源。<br>
因此，即使是训练非英文主导的大语言模型时（如中英双语大模型），不仅要保留特定目标语言数据，还需要同时保留英文高质量数据。

+ `基于简单统计指标的过滤`<br>
为了识别高质量的文本数据，可以使用语料中标点符号分布、符号与单词比率、句子长度等特征来衡量`文本质量`，并过滤低质量数据。<br>
除了这些统计特征以外，也可以利用`困惑度`（Perplexity）等文本生成的评估指标来检测和删除表达不自然的句子。<br>
1）针对网页数据，过滤任何具有超过 100 个重复单词或句子的文档（来源：Dolma）<br>
2）针对网页数据，过滤符号和词元比大于 0.1 的文档（来源：Gopher）<br>
3）针对论坛数据，过滤掉任何点赞数少于 3 的用户评论（来源：Dolma）<br>
4）利用已有的语言模型计算文档困惑度，并以此作为文档过滤的依据（来源：Dolma）<br>
5）训练 FastText 分类器来检测和删除有毒或仇恨言论的内容（来源：Dolma）<br>

+ `基于关键词的过滤`<br>
在收集到的预训练语料中，可能会存在着大量的重复文本模式，诸如常见的 HTML 标签、超链接以及各种模板等。<br>
进一步，这些语料中还可能包含了一些具有攻击性、冒犯性的不良信息。<br>
针对不同的语料来源以及应用场景，我们可以制定精准的清洗规则，结合相应的关键词集合，对文本进行扫描过滤。<br>
1）针对维基百科数据，过滤掉任何拥有少于 25 个 UTF-8 单词的页面。（来源：Dolma）
2）针对网页数据，过滤掉 HTML 标签（来源：RefinedWeb）
3）针对网页数据，过滤掉任何不含有 the, be, to, of, and, that, have, with 词汇的文档 （来源：Gopher）
4）针对所有数据，过滤掉如电话号码，邮箱地址，以及 IP 地址等隐私信息（来源：Dolma）

##### 基于分类器的方法

除了利用上述启发式的规则，我们也可以训练用于判别数据质量的文本分类器，进行预训练语料的清洗。
在选取样本时，可以将维基百科等高质量数据作为`正样本`，同时从网页中筛选出含有不良内容或低质量数据的样本作为`负样本`。
为了减少数据的误筛，可以使用`多个分类器进行联合过滤或召回`，从而来实现对低质量文本的高可信过滤。 

目前常用来实现分类器的方法包括轻量级模型（如 `FastText` 等）、可微调的预训练语言模型（如 BERT、BART 或者 LLaMA 等）以及闭源大语言模型 API（如GPT-4、Claude 3）。 
这三个方法各自具有不同的优缺点：轻量级模型效率较高，但是分类的准确率和精度可能受限于模型能力；预训练语言模型可以针对性微调，但是分类性能的通用性和泛化性仍然有一定的限制；闭源大语言模型的能力较强， 但是无法灵活针对任务进行适配，而且用于预训练数据清洗需要花费较高的成本。

为了平衡效率与准确性，可以针对具体数据集合进行清洗策略的灵活组合。例如，可以首先利用启发式规则进行初步筛选，以快速排除不符合要求的文档，随后再采用分类器方法进一步精细过滤，确保最终筛选出的语料具有较好的文本质量。


#### 4.2.2 敏感内容过滤
除了去除低质量内容，收集到的数据还可能包括`有毒内容`或`隐私信息`，需要进一步进行更为细致的过滤和处理。

+ `过滤有毒内容`<br>
可以采用基于分类器的过滤方法。 <br>
Jigsaw 评论数据集提供了用于训练毒性分类器的数据。（160K 条论坛评论数据，六个类别）<br>
通过设置合理的阈值，训练完成的分类器将能够有效识别并过滤掉含有有毒内容的信息。
使用高阈值时去除的数据会过少，语料中未过滤掉的有毒内容会导致模型在下游任务上的性能下降；而低阈值则会过滤更多的有毒内容，但同时也会造成大量数据的浪费。
考虑到后续的预处理操作（如质量筛选、去重等）同样能够有效剔除有害内容，Dolma 选择为分类器设定了一个相对较高的阈值（0.4），从而保留更多的候选数据。

+ `过滤隐私内容`<br>
Dolma 采用了基于规则的方法来过滤数据集中的隐私内容，主要标注了三类敏感信息：邮箱地址、IP 地址以及电话号码。<br>
如果某个文档中的隐私信息少于五条，Dolma 会使用特定的词元（如“|||EMAIL_ADDRESS|||”、“|||PHONE_NUMBER|||” 和“|||IP_ADDRESS|||”）来替换这些信息，以保护用户的隐私。
然而，如果文档中的隐私信息达到六条或更多，Dolma 会选择直接删除整个文档。这是因为当文档中频繁出现隐私信息时，很可能还隐藏着其他未标注的敏感内容。

#### 4.2.3 数据去重

由于大语言模型具有较强的`数据拟合`与`记忆能力`，很容易习得训练数据中的重复模式，可能导致对于这些模式的过度学习。
预训练语料中出现的重复低质量数据可能诱导模型在生成时频繁输出类似数据，进而影响模型的性能。此外，这些数据也可能导致训练过程的不稳定（训练损失震荡），可能导致训练过程崩溃。此外，为了避免数据集污染问题，还需要从预训练数据集中删除在测试集中可能出现的重复或者相关文本，从而防止训练集和测试集之间的重叠。 总体来说，去重算法的设计可以基于不同的计算粒度以及匹配方法。

+ `计算粒度`<br>
去重可以在句子级别、文档级别和数据集级别等多种粒度上进行。现有的数据集往往采用多阶段、多粒度的方式来实现高效的去重。首先针对数据集和文档级别进行去重，旨在去除那些具有高度相似甚至完全一致内容的文档，例如新闻数据集中包含相同的新闻文档。随后，可以进一步在句子级别实现更为精细的去重。

+ `用于去重的匹配方法`<br>
在去重过程中，可以使用精确匹配算法（即每个字符完全相同）或近似匹配算法（基于某种相似性度量）。对于精确匹配来说，通常使用后缀数组来匹配最小长度的完全相同子串。对于近似匹配来说，可以采用局部敏感哈希（Locality-Sensitive Hashing, LSH）算法，如最小哈希（MinHash）来实现。

#### 4.2.4 数据对预训练效果的影响
已有的研究表明，基于含有噪音、有毒和重复数据的低质量语料库进行预训练，会严重损害模型性能。

##### 数据数量的影响
整体上，`语言模型的性能会随着训练数据数量的增加而提升`，符合扩展法则。
一些更小尺寸的语言模型也使用了高达 1T 级别的数据进行了训练，发现其仍然没有达到语言模型能够学习的数据量上限。数据量的扩展性本质上来源于 Transformer 模型的可扩展性，这也是大语言模型能够取得成功最为关键的基础要素。

##### 数据质量的影响
在获取充足数量的预训练数据后，数据质量直接决定了模型的实际性能。通过显著提升数据质量，使得语言模型在参数、数据、算力更加节约的情况下就能展现出与更大规模模型相匹敌甚至更为优异的性能。

+ `整体影响`<br>
大语言模型所掌握的知识信息也来源于预训练数据，这意味着如果模型在包含事实性错误的、过时的数据上进行训练，那么它在处理相关主题时可能会产生不准确或虚假的信息，这种现象被称为“幻象” 。

+ `重复数据`<br>
重复数据也可能导致“双下降现象”，即模型训练损失先经历下降然后出现升高再下降的现象。此外，重复数据可能会降低大语言模型利用上下文中信息的能力。这会削弱模型在上下文学习中的泛化能力，使其难以适应各种复杂的语言环境和任务需求。随着模型参数规模的不断增加，公开可获取的数据将很快接近采集枯竭的状态，可以使用大语言模型对于稀缺数据进行改写或者针对性的生成。

+ `有偏、有毒、隐私内容`<br>
如果训练数据中包含有毒内容，模型则可能会产生侮辱性、攻击性或其他有害的输出；而在含有隐私内容的数据上训练可能会导致模型在输出中无意中泄露或利用个人数据。因此，在训练大语言模型之前，需要通过严格的数据过滤和预处理方法来尽量减少有偏见、有毒或包含隐私信息的数据。

##### 数据集污染
在进行模型评测时，可能会发现某些评估基准所包含的数据，实际上已出现在预训练数据或者微调数据中，这种现象被称为基准泄漏或`数据集污染`。数据集污染问题可能导致模型在与测试数据集相关甚至高度重合的语料上进行训练，从而原本用于衡量模型在少样本或零样本场景下的性能评测，转变为了领域内的测试任务。这种情况破坏了评估集合构建的初衷，使得不同模型之间的对比失去了公平性。


### 4.3 词元化（分词）
词元化（Tokenization）旨在将原始文本分割成模型可识别和建模的词元序列，作为大语言模型的输入数据。
最近，子词分词器（Subword Tokenizer）被广泛应用于基于 Transformer 的语言模型中，包括 BPE 分词、WordPiece 分词和 Unigram 分词三种常见方法。

#### 4.3.1 BPE 分词
在 1994 年，BPE 算法被提出，最早用于通用的数据压缩。
随后，自然语言处理领域的研究人员将其进行适配，并应用于文本分词。
BPE 算法从一组基本符号（例如字母和边界字符）开始，迭代地寻找语料库中的两个相邻词元，并将它们替换为新的词元，这一过程被称为`合并`。
合并的选择标准是计算两个连续词元的共现频率，也就是每次迭代中，最频繁出现的一对词元会被选择与合并。
合并过程将一直持续达到预定义的词表大小。

字节级别的 BPE（Byte-level BPE, B-BPE）是 BPE 算法的一种拓展。
它将字节视为合并操作的基本符号，从而可以实现更细粒度的分割，且解决了未登录词问题。
具体来说，如果将所有 Unicode 字符都视为基本字符，那么包含所有可能基本字符的基本词表会非常庞大（例如将中文的每个汉字当作一个基本字符）。
而将字节作为基本词表可以设置基本词库的大小为 256，同时确保每个基本字符都包含在词汇中。 
例如，GPT-2 的词表大小为 50,257 ，包括 256 个字节的基本词元、一个特殊的文末词元以及通过 50,000 次合并学习到的词元。

#### 4.3.2 WordPiece 分词
WordPiece 是谷歌内部非公开的分词算法，最初是由谷歌研究人员在开发语音搜索系统时提出的。
随后，在 2016 年被用于机器翻译系统，并于 2018 年被 BERT 采用作为分词器。
WordPiece 分词和 BPE 分词的想法非常相似，都是通过迭代合并连续的词元，但是合并的选择标准略有不同。
在合并前，`WordPiece`分词算法会首先训练一个语言模型，并用这个语言模型对所有可能的词元对进行评分。然后，在每次合并时，它都会选择使得训练数据的似然性增加最多的词元对。
与 BPE 类似，Word Piece 分词算法也是从一个小的词汇表开始，其中包括模型使用的特殊词元和初始词汇表。
由于它是通过添加前缀（如 BERT 的##）来识别子词的，因此每个词的初始拆分都是将前缀添加到词内的所有字符上。
举例来说，“word”会被拆分为：“w##o ##r ##d”。与 BPE 方法的另一个不同点在于，WordPiece 分词算法并不选择最频繁的词对，而是使用下面的公式为每个词对计算分数：

$$
得分=\frac{词对出现的频率}{第一个词出现的频率\times 第二个词出现的频率} \\
$$

#### 4.3.3 Unigram 分词
与 BPE 分词和 WordPiece 分词不同，Unigram 分词方法从语料库的一组足够大的字符串或词元初始集合开始，迭代地删除其中的词元，直到达到预期的词表大小。
它假设从当前词表中删除某个词元，并计算训练语料的似然增加情况，以此来作为选择标准。
这个步骤是基于一个训练好的一元语言模型来进行的。
为估计一元语言模型，它采用期望最大化（Expectation–Maximization, EM）算法：
在每次迭代中，首先基于旧的语言模型找到当前最优的分词方式，然后重新估计一元概率从而更新语言模型。
这个过程中一般使用动态规划算法（即`维特比算法`，Viterbi Algorithm）来高效地找到语言模型对词汇的最优分词方式。采用这种分词方法的代表性模型包括 T5 和 mBART。


#### 4.3.4 分词器的选用
虽然直接使用已有的分词器较为方便（例如 OPT和 GPT-3使用了 GPT-2的分词器），但是使用为预训练语料专门训练或设计的分词器会更加有效，尤其是对于那些混合了多领域、多语言和多种格式的语料。
最近的大语言模型通常使用 SentencePiece 代码库为预训练语料训练定制化的分词器，这一代码库支持字节级别的 BPE 分词和 Unigram 分词。

首先，分词器必须具备无损重构的特性，即其分词结果能够准确无误地还原为原始输入文本。其次，分词器应具有高压缩率，即在给定文本数据的情况下，经过分词处理后的词元数量应尽可能少，从而实现更为高效的文本编码和存储。具体来说，压缩比可以通过将原始文本的 UTF-8 字节数除以分词器生成的词元数（即每个词元的平均字节数）来计算：

$$
压缩率=\frac{UTF-8字节数}{词元数} \\
$$

值得注意的是，在扩展现有的大语言模型（如继续预训练或指令微调）的同时，还需要意识到原始分词器可能无法较好地适配实际需求。
此外，为进一步提高某些特定能力（如数学能力），还可能需要针对性地设计分词器。例如，BPE 分词器可能将整数 7,481 分词为“7 481”，而将整数 74,815 分词为“748 15”。

### 4.4 数据调度
通常来说，数据调度（Data Scheduling）主要关注两个方面：`各个数据源的混合比例`以及`各数据源用于训练的顺序`（称为数据课程，Data Curriculum）。

<img src="images/图 4.3 预训练大语言模型时数据调度的示意图.png" width="60%" height="60%" alt="">

#### 4.4.1 数据混合
由于不同数据源与大语言模型某些特定能力的学习具有紧密的联系，因此设置合适的数据混合比例非常重要。
数据混合通常在数据集合层面上设置（即整个预训练数据的整体分布），也可以在不同训练阶段采用不同的混合数据比例。
在预训练期间，将根据混合比例从不同数据源中采样数据：数据源的权重越大，从中选择的数据就越多。
进一步，可能会对每个数据源的全部数据进行上采样或下采样，以创建特定的数据混合集合作为预训练数据。

在实践中，数据混合通常是根据经验确定的，下面汇总了几种常见的数据混合策略。

+ `增加数据源的多样性`<br>
为了提升大语言模型的整体能力，增加数据源异质性（即包括多样化的数据源）能够有助于改进大语言模型在下游任务中的综合表现。 如网页数据、各类型书籍、代码数据等。

+ `优化数据混合`<br>
为了减少对于目标任务的依赖，DoReMi首先使用给定的初始领域权重训练一个小型参考模型，然后在每次迭代过程中，使用当前的领域权重计算得到数据比例，用其训练另一个小型代理模型。然后通过比较两个模型损失值的差距，对该域数据的采样权重进行优化。具体来说，对于代理模型“未较好习得的”数据域，所分配的域权重将会被增加。最后，通过多轮迭代，代理模型最终的域权重将被应用于大语言模型训练。 此外，一个更为简单的实践方法是，训练几个具有不同数据混合配比的小型语言模型，并选择获得最理想性能的数据混合配比。然而，这个方法的一个假设是，如果以类似的方式训练，小模型会在模型能力或行为上与大模型相似，这在实际中可能并不总是成立。

+ `优化特定能力`<br>
大语言模型的模型能力在很大程度上取决于数据选择和配比，可以通过增加特定数据源的比例来增强某些对应的模型能力。
为了增强大语言模型的特定能力（如数学和编码），或开发专用的大语言模型，一种常见的方法是采用`多阶段训练方法`，例如可以在连续两个阶段分别安排通用数据和任务特定数据。

#### 4.4.2 数据课程

除了设置有效的数据混合配比外，在训练过程中对于预训练数据的顺序进行合适的安排也比较重要。
具体来说，`数据课程`是指按照特定的顺序安排预训练数据进行模型的训练。 更广泛地说，它可以指训练期间在不同阶段使用不同的数据源混合配比。

为了设定合适的数据课程，一种实用方法是基于专门构建的评测基准监控大语言模型的关键能力的学习过程，然后在预训练期间动态调整数据的混合配比。
由于预训练阶段需要耗费大量的计算资源，目前针对数据课程的研究工作主要集中在`继续预训练`（Continual Pre-training）这一方面。
相关研究表明，为了学习某些特定的技能，按照技能依赖顺序编排对应数据集的学习方法（例如，基本技能 → 目标技能）比直接在相关的特定语料库上学习效果更好。

+ `代码能力`<br>
CodeLLaMA（基于LLaMA2）能够更为有效地执行代码任务。采用的数据为：2T 通用词元 → 500B 代码密集型词元。<br>
CodeLLaMA-Python：2T 通用词元 → 500B 代码相关的词元 → 100B Python 代码相关的词元。

+ `数学能力`<br>
Llemma（数学大语言模型，CodeLLaMA作为基座模型）：2T 通用词元 → 500B 代码相关的词元 → 50∼200B 数学相关的词元。

+ `长文本能力`<br>
CodeLLaMA 将 LLaMA-2 的上下文窗口从 4K 扩展到了 100K，所采用的数据课程为：2.5T词元，4K上下文窗口 → 20B词元，16K上下文窗口。<br>
通过使用这种训练序列长度由短到长的数据课程，能够使模型获得较好的长文本建模能力，同时可以节省长文本模型的训练时间。
