{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a786c77c",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 第三章 储存\n",
    "\n",
    " - [一、对话缓存储存](#二、对话缓存储存)\n",
    "     - [1.1 初始化对话模型](#1.1-初始化对话模型)\n",
    "     - [1.2 对话](#1.2-对话)\n",
    "     - [1.3 查看储存缓存](#1.3-查看储存缓存)\n",
    "     - [1.4 直接添加内容到储存缓存](#1.4-直接添加内容到储存缓存)\n",
    "     - [1.5 总结](#1.5-总结)\n",
    " - [二、对话缓存窗口储存](#二、对话缓存窗口储存)\n",
    "     - [2.1 添加两轮对话到窗口储存](#2.1-添加两轮对话到窗口储存)\n",
    "     - [32.2 在对话链中应用窗口储存](#2.2-在对话链中应用窗口储存)\n",
    " - [三、对话token缓存储存](#三、对话token缓存储存)\n",
    " - [四、对话摘要缓存储存](#四、对话摘要缓存储存)\n",
    "     - [4.1 使用对话摘要缓存储存](#4.1-使用对话摘要缓存储存)\n",
    "     - [4.2 基于对话摘要缓存储存的对话链](#4.2-基于对话摘要缓存储存的对话链)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10db6f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "当你与那些语言模型进行交互的时候，他们不会记得你之前和他进行的交流内容，这在我们构建一些应用程序（如聊天机器人）的时候，是一个很大的问题 -- 显得不够智能！因此，在本节中我们将介绍 LangChain 中的储存模块，即如何将先前的对话嵌入到语言模型中的，使其具有连续对话的能力。\n",
    "\n",
    "当使用 LangChain 中的储存(Memory)模块时，它可以帮助保存和管理历史聊天消息，以及构建关于特定实体的知识。这些组件可以跨多轮对话储存信息，并允许在对话期间跟踪特定信息和上下文。\n",
    "\n",
    "LangChain 提供了多种储存类型。其中，缓冲区储存允许保留最近的聊天消息，摘要储存则提供了对整个对话的摘要。实体储存 则允许在多轮对话中保留有关特定实体的信息。这些记忆组件都是模块化的，可与其他组件组合使用，从而增强机器人的对话管理能力。储存模块可以通过简单的API调用来访问和更新，允许开发人员更轻松地实现对话历史记录的管理和维护。\n",
    "\n",
    "此次课程主要介绍其中四种储存模块，其他模块可查看文档学习。\n",
    "- 对话缓存储存 (ConversationBufferMemory）\n",
    "- 对话缓存窗口储存 (ConversationBufferWindowMemory）\n",
    "- 对话令牌缓存储存 (ConversationTokenBufferMemory）\n",
    "- 对话摘要缓存储存 (ConversationSummaryBufferMemory）\n",
    "\n",
    "在LangChain中，储存 指的是大语言模型（LLM）的短期记忆。为什么是短期记忆？那是因为LLM训练好之后 (获得了一些长期记忆)，它的参数便不会因为用户的输入而发生改变。当用户与训练好的LLM进行对话时，LLM会暂时记住用户的输入和它已经生成的输出，以便预测之后的输出，而模型输出完毕后，它便会“遗忘”之前用户的输入和它的输出。因此，之前的这些信息只能称作为LLM的短期记忆。  \n",
    "  \n",
    "为了延长LLM短期记忆的保留时间，则需要借助一些外部储存方式来进行记忆，以便在用户与LLM对话中，LLM能够尽可能的知道用户与它所进行的历史对话信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297dcd5",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 一、对话缓存储存\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3e4e9-7a6a-4a09-9ac3-0096a67849c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 初始化对话模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3577aaff-7edb-40b0-866a-e407e63d55e0",
   "metadata": {
    "height": 98,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "\n",
    "API_KEY = os.environ.get(\"CHAT_ANYWHERE_API_KEY\")\n",
    "BASE_URL = \"https://api.chatanywhere.com.cn/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77d37ab-1f75-4ae8-8d7c-5066773ead81",
   "metadata": {
    "height": 133,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ConversationChain(verbose=True, llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10a040a90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10a011b40>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_api_base='https://api.chatanywhere.com.cn/v1', openai_proxy=''))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, base_url=BASE_URL, api_key=API_KEY)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747cb539-abc4-4e47-8cb9-1ee608ab07fc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e48462-7a92-4842-bdaa-2a478ba2252c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "' 你好皮皮鲁！我是一个AI助手，很高兴认识你。你有什么问题想问我吗？'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"你好, 我叫皮皮鲁\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed6e97a-d7ea-4188-a6d7-f91d2a29d14a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI:  你好皮皮鲁！我是一个AI助手，很高兴认识你。你有什么问题想问我吗？\n",
      "Human: 1+1等于多少？\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "'1加1等于2。这是一个基本的数学问题，答案是2。你还有其他问题吗？'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"1+1等于多少？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5dfe488-2758-42c7-9c20-e483b4c22ab8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI:  你好皮皮鲁！我是一个AI助手，很高兴认识你。你有什么问题想问我吗？\n",
      "Human: 1+1等于多少？\n",
      "AI: 1加1等于2。这是一个基本的数学问题，答案是2。你还有其他问题吗？\n",
      "Human: 我叫什么名字？\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "'你叫皮皮鲁。你之前告诉我你的名字是皮皮鲁。有什么其他问题想问我吗？'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"我叫什么名字？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9822e-943d-4905-a1f8-a0d28c215d60",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 查看储存缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0795580f-b6b6-47e0-8882-26fe204560bd",
   "metadata": {
    "height": 31,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 你好, 我叫皮皮鲁\n",
      "AI:  你好皮皮鲁！我是一个AI助手，很高兴认识你。你有什么问题想问我吗？\n",
      "Human: 1+1等于多少？\n",
      "AI: 1加1等于2。这是一个基本的数学问题，答案是2。你还有其他问题吗？\n",
      "Human: 我叫什么名字？\n",
      "AI: 你叫皮皮鲁。你之前告诉我你的名字是皮皮鲁。有什么其他问题想问我吗？\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48942759-8afb-4aed-80c5-a48952a2b0c0",
   "metadata": {
    "height": 31,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 你好, 我叫皮皮鲁\\nAI:  你好皮皮鲁！我是一个AI助手，很高兴认识你。你有什么问题想问我吗？\\nHuman: 1+1等于多少？\\nAI: 1加1等于2。这是一个基本的数学问题，答案是2。你还有其他问题吗？\\nHuman: 我叫什么名字？\\nAI: 你叫皮皮鲁。你之前告诉我你的名字是皮皮鲁。有什么其他问题想问我吗？'}\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3778e-5fbf-43ed-9df1-d57d98ec6fb0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 直接添加内容到储存缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "147b2c30-1662-4b49-aaf8-c228428e5cc6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'history': 'Human: 你好，我叫皮皮鲁\\nAI: 你好啊，我叫鲁西西'}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"你好，我叫皮皮鲁\"}, {\"output\": \"你好啊，我叫鲁西西\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b5e27f0-dad7-41b6-9326-bebf6299638f",
   "metadata": {
    "height": 64,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfa7c555-06ab-4906-b3dc-906f789e08f5",
   "metadata": {
    "height": 31,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'history': 'Human: 你好，我叫皮皮鲁\\nAI: 你好啊，我叫鲁西西\\nHuman: Not much, just hanging\\nAI: Cool'}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deb33de8-37ea-4180-a73e-0fc456b14eb0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'history': 'Human: 你好，我叫皮皮鲁\\nAI: 你好啊，我叫鲁西西\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: 很高兴和你成为朋友！\\nAI: 是的，让我们一起去冒险吧！'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"很高兴和你成为朋友！\"}, {\"output\": \"是的，让我们一起去冒险吧！\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10146f1a-0114-4902-8122-d19ae6f7c461",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759b6bc-edb2-4cfe-b0f8-1bf6c4d796f9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "当我们在使用大型语言模型进行聊天对话时，**大型语言模型本身实际上是无状态的。语言模型本身并不记得到目前为止的历史对话**。每次调用API结点都是独立的。储存(Memory)可以储存到目前为止的所有术语或对话，并将其输入或附加上下文到LLM中用于生成输出。如此看起来就好像它在进行下一轮对话的时候，记得之前说过什么。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98e9ff",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 二、对话缓存窗口储存\n",
    "  \n",
    "随着对话变得越来越长，所需的内存量也变得非常长。将大量的tokens发送到LLM的成本，也会变得更加昂贵,这也就是为什么API的调用费用，通常是基于它需要处理的tokens数量而收费的。\n",
    "  \n",
    "针对以上问题，LangChain也提供了几种方便的储存方式来保存历史对话。其中，对话缓存窗口储存只保留一个窗口大小的对话。它只使用最近的n次交互。这可以用于保持最近交互的滑动窗口，以便缓冲区不会过大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641477a4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 添加两轮对话到窗口储存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ea6233e",
   "metadata": {
    "height": 47,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'history': 'Human: 很高兴和你成为朋友！\\nAI: 是的，让我们一起去冒险吧！'}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# k=1表明只保留一个对话记忆\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({\"input\": \"你好，我叫皮皮鲁\"}, {\"output\": \"你好啊，我叫鲁西西\"})\n",
    "memory.save_context({\"input\": \"很高兴和你成为朋友！\"}, {\"output\": \"是的，让我们一起去冒险吧！\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bda148",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 在对话链中应用窗口储存\n",
    "\n",
    "注意此处！由于这里用的是一个窗口的记忆，因此只能保存一轮的历史消息，因此AI并不能知道你第一轮对话中提到的名字，他最多只能记住上一轮（第二轮）的对话信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4087bc87",
   "metadata": {
    "height": 133,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 你好皮皮鲁！我是一个AI助手，很高兴认识你。你有什么问题想问我吗？\n",
      "1加1等于2。这是一个基本的数学问题，答案是2。你还有其他问题吗？\n"
     ]
    },
    {
     "data": {
      "text/plain": "'抱歉，我不知道你的名字。我只是一个人工智能程序，无法知道你的个人信息。有其他问题我可以帮你解答吗？'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, base_url=BASE_URL, api_key=API_KEY)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
    "print(conversation.predict(input=\"你好, 我叫皮皮鲁\"))\n",
    "print(conversation.predict(input=\"1+1等于多少？\"))\n",
    "conversation.predict(input=\"我叫什么名字？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2931b92",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 三、对话token缓存储存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5b4c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用对话token缓存记忆，内存将限制保存的token数量。如果token数量超出指定数目，它会切掉这个对话的早期部分\n",
    "以保留与最近的交流相对应的token数量，但不超过token限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f6d063c",
   "metadata": {
    "height": 31,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q tiktoken            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb9020ed",
   "metadata": {
    "height": 81,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'history': 'AI: 轻舟已过万重山。'}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "# 指定llm类型的token计算方式\n",
    "llm = ChatOpenAI(temperature=0.0, base_url=BASE_URL, api_key=API_KEY)\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n",
    "memory.save_context({\"input\": \"朝辞白帝彩云间，\"}, {\"output\": \"千里江陵一日还。\"})\n",
    "memory.save_context({\"input\": \"两岸猿声啼不住，\"}, {\"output\": \"轻舟已过万重山。\"})\n",
    "\n",
    "# 前面超出的的token已经被舍弃了！！！\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb08ef4a-876f-422a-81f9-4805288e5955",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "补充"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d918b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "ChatGPT使用一种基于字节对编码（Byte Pair Encoding，BPE）的方法来进行tokenization（将输入文本拆分为token）。BPE是一种常见的tokenization技术，它将输入文本分割成较小的子词单元。 \n",
    "\n",
    "OpenAI在其官方GitHub上公开了一个最新的开源Python库 [tiktoken](https://github.com/openai/tiktoken)，这个库主要是用来计算tokens数量的。相比较HuggingFace的tokenizer，其速度提升了好几倍。\n",
    "在线的token计算方式：[OpenAI Platform Tokenizer](https://platform.openai.com/tokenizer)\n",
    "\n",
    "具体token计算方式,特别是汉字和英文单词的token区别，具体课参考[知乎文章](https://www.zhihu.com/question/594159910) 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff55d5d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 四、对话摘要缓存储存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d39b83a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "对话摘要缓存储存，**使用LLM编写到目前为止历史对话的摘要**，并将其保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572ef39",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1 使用对话摘要缓存储存\n",
    "\n",
    "创建一个长字符串，其中包含某人的日程安排"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c07922b",
   "metadata": {
    "height": 31,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'history': 'System: The human and AI introduce themselves in Chinese and become friends. They plan their day, including a meeting with the product team, working on LangChain, and having lunch with a customer interested in AI. The AI emphasizes the importance of being prepared to showcase the latest LLM examples.'}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "\n",
    "# 创建一个长字符串\n",
    "schedule = \"在八点你和你的产品团队有一个会议。 \\\n",
    "你需要做一个PPT。 \\\n",
    "上午9点到12点你需要忙于LangChain。\\\n",
    "Langchain是一个有用的工具，因此你的项目进展的非常快。\\\n",
    "中午，在意大利餐厅与一位开车来的顾客共进午餐 \\\n",
    "走了一个多小时的路程与你见面，只为了解最新的 AI。 \\\n",
    "确保你带了笔记本电脑可以展示最新的 LLM 样例.\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, base_url=BASE_URL, api_key=API_KEY)\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)  # 调用API生成摘要\n",
    "memory.save_context({\"input\": \"你好，我叫皮皮鲁\"}, {\"output\": \"你好啊，我叫鲁西西\"})\n",
    "memory.save_context({\"input\": \"很高兴和你成为朋友！\"}, {\"output\": \"是的，让我们一起去冒险吧！\"})\n",
    "memory.save_context({\"input\": \"今天的日程安排是什么？\"}, {\"output\": f\"{schedule}\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e29a956-607a-4247-9eb5-01285a370991",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.2 基于对话摘要缓存储存的对话链\n",
    "基于上面的对话摘要缓存储存，新建一个对话链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52696c8c",
   "metadata": {
    "height": 31,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI introduce themselves in Chinese and become friends. They plan their day, including a meeting with the product team, working on LangChain, and having lunch with a customer interested in AI. The AI emphasizes the importance of being prepared to showcase the latest LLM examples.\n",
      "Human: 展示什么样的样例最好呢？使用中文回答\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "'最好展示一些涉及自然语言处理和语言生成的例子，比如文本摘要、对话生成和情感分析等。这些例子能够展示LangChain在处理自然语言任务方面的强大能力。'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"展示什么样的样例最好呢？使用中文回答\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85bba1f8",
   "metadata": {
    "height": 31,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'history': 'System: The human and AI introduce themselves in Chinese and become friends. They plan their day, including a meeting with the product team, working on LangChain, and having lunch with a customer interested in AI. The AI emphasizes the importance of being prepared to showcase the latest LLM examples.\\nHuman: 展示什么样的样例最好呢？使用中文回答\\nAI: 最好展示一些涉及自然语言处理和语言生成的例子，比如文本摘要、对话生成和情感分析等。这些例子能够展示LangChain在处理自然语言任务方面的强大能力。'}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})  # 摘要记录更新了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}